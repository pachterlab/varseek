{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import varseek as vk\n",
    "import os\n",
    "import sys\n",
    "import anndata as ad\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from varseek.utils import build_random_genome_read_df, fasta_to_fastq, create_mutant_t2g, fasta_to_fastq, find_genes_with_aligned_reads_for_kb_extract, calculate_metrics, create_stratified_metric_bar_plot, create_venn_diagram, get_header_set_from_fastq, plot_histogram, synthetic_data_summary_plot, plot_basic_bar_plot_from_dict, draw_confusion_matrix, check_for_read_kmer_in_mcrs, make_bus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir_notebook=\"/home/jrich/data/varseek_data_fresh/vk_sim_2024nov24_500000mcrs_k59_nov16\" #!!! change for each run\n",
    "number_of_mutations_to_sample = 500000\n",
    "# Paths\n",
    "mutation_metadata_df = \"/home/jrich/data/varseek_data_fresh/vk_build_pipeline_grch37_nov16/mutation_metadata_df_updated_vk_info_exploded.csv\"\n",
    "mutation_index = \"/home/jrich/data/varseek_data_fresh/vk_build_pipeline_grch37_nov16/mutation_reference.idx\"\n",
    "mutation_t2g = \"/home/jrich/data/varseek_data_fresh/vk_build_pipeline_grch37_nov16/t2g_filtered.txt\"\n",
    "\n",
    "\n",
    "\n",
    "read_length = 150\n",
    "strand = None  # None for strand-agnostic (randomly-selected), \"f\" for forward, \"r\" for reverse, \"both\" for both - make sure this matches the reference genome (vk build command) - strand = True -> \"f\" or \"r\" here; strand = False -> None or \"both\" here - note that the strand is randomly selected per *transcript*, such that all drawn reads will come from the same strand no matter what\n",
    "add_noise=False\n",
    "error_rate=0\n",
    "max_errors=0\n",
    "seq_id_column=\"seq_ID\"\n",
    "mut_column=\"mutation\"\n",
    "threads = 32\n",
    "check_reason_for_read_fn = False\n",
    "\n",
    "k = 59  # w=54 - even if I will use k=59 in my experiments, my synthetic data is designed so that w=(read_length-1), meaning that the mutation can be anywhere on the read from the first to the last nucleotide, and thus I cannot catch these edge mutations if k > (w+1) - if I want to run where k > w+1, then adjust w internal to vk sim accoringly\n",
    "w = 54\n",
    "\n",
    "sequences = \"/home/jrich/data/varseek_data_fresh/reference/ensembl_grch37_release93/Homo_sapiens.GRCh37.cdna.all.fa\"\n",
    "reference_out_dir = \"/home/jrich/data/varseek_data_fresh/reference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir_vk_build = f\"{out_dir_notebook}/vk_build\"\n",
    "\n",
    "synthetic_reads_fastQ = f\"{out_dir_notebook}/synthetic_reads.fq\"\n",
    "read_df_out = f\"{out_dir_notebook}/synthetic_reads.csv\"\n",
    "mutation_metadata_df_out = f\"{out_dir_notebook}/mutation_metadata_df_updated_vk_info_exploded_with_synthetic_read_info.csv\"\n",
    "\n",
    "kb_count_out = f\"{out_dir_notebook}/kb_count_out\"\n",
    "kb_count_out_mm = f\"{out_dir_notebook}/kb_count_out_mm\"\n",
    "\n",
    "\n",
    "seed=42\n",
    "read_df = None\n",
    "mutation_metadata_df_path = mutation_metadata_df\n",
    "adata_path = f\"{kb_count_out}/counts_unfiltered/adata.h5ad\"\n",
    "\n",
    "read_df_out_updated = read_df_out.replace(\".csv\", \"_updated.csv\")\n",
    "unique_mcrs_df_out = f\"{out_dir_notebook}/unique_mcrs_df.csv\"\n",
    "\n",
    "plot_output_folder = f\"{out_dir_notebook}/plots\"\n",
    "os.makedirs(plot_output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run vk ref or notebook 1_1 (vk build, vk info with save_exploded_df=True, vk filter, kb ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple synthetic reads - erase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!!! TEMP\n",
    "# import varseek\n",
    "# mutation_metadata_df_path = mutation_metadata_df\n",
    "# mutation_metadata_df = pd.read_csv(mutation_metadata_df_path)\n",
    "\n",
    "# sim_data_df_path = \"/home/jrich/data/varseek_data_fresh/vk_sim_2024nov24_temp2/vk_build/sim_data_df.csv\"\n",
    "# sim_data_df = pd.read_csv(sim_data_df_path, usecols=[\"header\", \"mutant_sequence\", \"wt_sequence\"])\n",
    "\n",
    "# sim_data_df.rename(\n",
    "#     columns={\n",
    "#         \"mutant_sequence\": \"mutant_sequence_read_parent\",\n",
    "#         \"wt_sequence\": \"wt_sequence_read_parent\",\n",
    "#     },\n",
    "#     inplace=True,\n",
    "# )\n",
    "\n",
    "# sim_data_df[\"mutant_sequence_read_parent_rc\"] = sim_data_df[\"mutant_sequence_read_parent\"].apply(varseek.varseek_build.reverse_complement)\n",
    "# sim_data_df[\"mutant_sequence_read_parent_length\"] = sim_data_df[\"mutant_sequence_read_parent\"].str.len()\n",
    "\n",
    "# sim_data_df[\"wt_sequence_read_parent_rc\"] = sim_data_df[\"wt_sequence_read_parent\"].apply(varseek.varseek_build.reverse_complement)\n",
    "# sim_data_df[\"wt_sequence_read_parent_length\"] = sim_data_df[\"wt_sequence_read_parent\"].str.len()\n",
    "\n",
    "# mutation_metadata_df = pd.merge(\n",
    "#     mutation_metadata_df,\n",
    "#     sim_data_df[\n",
    "#         [\n",
    "#             \"header\",\n",
    "#             \"mutant_sequence_read_parent\",\n",
    "#             \"mutant_sequence_read_parent_rc\",\n",
    "#             \"mutant_sequence_read_parent_length\",\n",
    "#             \"wt_sequence_read_parent\",\n",
    "#             \"wt_sequence_read_parent_rc\",\n",
    "#             \"wt_sequence_read_parent_length\",\n",
    "#         ]\n",
    "#     ],\n",
    "#     on=\"header\",\n",
    "#     how=\"left\",\n",
    "#     suffixes=(\"\", \"_read_parent\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Remove the module from sys.modules\n",
    "if 'varseek.sim' in sys.modules:\n",
    "    del sys.modules['varseek.sim']\n",
    "\n",
    "# Import the module and reload it\n",
    "import varseek.varseek_sim\n",
    "importlib.reload(varseek.varseek_sim)\n",
    "import varseek as vk\n",
    "\n",
    "# instead of using vk.sim() as before, I must explicitely use vk.varseek_sim.sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!! TEMP\n",
    "sample_type = \"all\"\n",
    "conditions = ['dlist-equal=none', 'mcrs_is_substring-isnottrue', 'pseudoaligned_to_human_reference_despite_not_truly_aligning-isnottrue']  # kmer_overlap_in_mcrs_reference-isnottrue\n",
    "\n",
    "\n",
    "if not os.path.exists(mutation_metadata_df_out) or not os.path.exists(read_df_out):\n",
    "    simulated_df_dict = vk.varseek_sim.sim(\n",
    "        mutation_metadata_df = mutation_metadata_df,\n",
    "        fastq_output_path = synthetic_reads_fastQ,\n",
    "        sample_type=sample_type,\n",
    "        number_of_mutations_to_sample=number_of_mutations_to_sample,\n",
    "        strand=strand,\n",
    "        number_of_reads_per_sample=\"all\",  # not used when number_of_reads_per_sample_m and number_of_reads_per_sample_w are provided\n",
    "        k=k,  # used in vk build with the respective index\n",
    "        w=w,  # used in vk build with the respective index\n",
    "        read_length=read_length,\n",
    "        seed=seed,\n",
    "        add_noise=add_noise,\n",
    "        error_rate=error_rate,\n",
    "        max_errors=max_errors,\n",
    "        with_replacement=False,\n",
    "        sequences=sequences,\n",
    "        mutation_metadata_df_path=mutation_metadata_df_path,\n",
    "        seq_id_column=seq_id_column,\n",
    "        mut_column=mut_column,\n",
    "        reference_out_dir=reference_out_dir,\n",
    "        out_dir_vk_build=out_dir_vk_build,\n",
    "        filters=conditions,\n",
    "        read_df_out=read_df_out,\n",
    "        mutation_metadata_df_out=mutation_metadata_df_out,\n",
    "    )\n",
    "\n",
    "    read_df, mutation_metadata_df = simulated_df_dict[\"read_df\"], simulated_df_dict[\"mutation_metadata_df\"]\n",
    "else:\n",
    "    read_df = pd.read_csv(read_df_out)\n",
    "    mutation_metadata_df = pd.read_csv(mutation_metadata_df_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex synthetic reads - uncomment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_list = [1, 2, 3, 4, 5, 6, 7, 8, 16, 32, 64, 128, 256, 512]\n",
    "# w_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 16, 32, 64, 128, 256, 512]\n",
    "# conditions = ['mutation_type-equal=substitution', 'mutation_type-notequal=substitution', 'dlist-equal=none', 'dlist-notequal=none', 'kmer_overlap_in_mcrs_reference-isfalse', 'kmer_overlap_in_mcrs_reference-istrue', 'is_near_splice_junction_10-isfalse', 'is_near_splice_junction_10-istrue']\n",
    "\n",
    "# random.shuffle(m_list)\n",
    "# random.shuffle(w_list)\n",
    "\n",
    "# if not os.path.exists(mutation_metadata_df_out) and not os.path.exists(read_df_out):\n",
    "#     for m in m_list:\n",
    "#         for w in w_list:\n",
    "#             for condition in conditions:\n",
    "#                 read_fq_path_specific = synthetic_reads_fastQ.replace(\".fq\", f\"_m{m}_w{w}_{condition.replace('=', '_')}.fq\")\n",
    "#                 condition_added_list = [condition, 'included_in_synthetic_reads-isnottrue']\n",
    "                \n",
    "#                 simulated_df_dict = vk.sim(\n",
    "#                     mutation_metadata_df = mutation_metadata_df,\n",
    "#                     fastq_output_path = read_fq_path_specific,\n",
    "#                     fastq_parent_path = synthetic_reads_fastQ,\n",
    "#                     read_df_parent=read_df,\n",
    "#                     sample_type=\"all\",\n",
    "#                     number_of_mutations_to_sample=number_of_mutations_to_sample,\n",
    "#                     strand=strand,\n",
    "#                     number_of_reads_per_sample=None,  # not used when number_of_reads_per_sample_m and number_of_reads_per_sample_w are provided\n",
    "#                     number_of_reads_per_sample_m=m,\n",
    "#                     number_of_reads_per_sample_w=w,\n",
    "#                     read_length=read_length,\n",
    "#                     seed=seed,\n",
    "#                     add_noise=add_noise,\n",
    "#                     error_rate=error_rate,\n",
    "#                     max_errors=max_errors,\n",
    "#                     with_replacement=False,\n",
    "#                     filters=condition_added_list,\n",
    "#                     sequences=sequences,\n",
    "#                     mutation_metadata_df_path=mutation_metadata_df_path,\n",
    "#                     seq_id_column=seq_id_column,\n",
    "#                     mut_column=mut_column,\n",
    "#                     reference_out_dir=reference_out_dir,\n",
    "#                     out_dir_vk_build=out_dir_vk_build,\n",
    "#                 )\n",
    "\n",
    "#                 mutation_metadata_df, read_df = simulated_df_dict['mutation_metadata_df'], simulated_df_dict['read_df']\n",
    "# else:\n",
    "#     read_df = pd.read_csv(read_df_out)\n",
    "#     mutation_metadata_df = pd.read_csv(mutation_metadata_df_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random genome - uncomment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do kv sim with random genome\n",
    "# mutation_metadata_df['start_position_for_which_read_contains_mutation_cdna'] = mutation_metadata_df['start_mutation_position_cdna'] - read_length + 1\n",
    "# mutation_metadata_df['start_position_for_which_read_contains_mutation_genome'] = mutation_metadata_df['start_mutation_position_genome'] - read_length + 1\n",
    "\n",
    "# number_of_random_reads_cdna = 10\n",
    "# number_of_random_reads_genome = 10\n",
    "\n",
    "# # read_df = build_random_genome_read_df(reference_fasta_file_path = reference_cdna_fasta, mutation_metadata_df = mutation_metadata_df, read_df = read_df, n = number_of_random_reads_cdna, read_length = read_length, input_type = \"transcriptome\", strand = None)\n",
    "# # read_df = build_random_genome_read_df(reference_fasta_file_path = reference_genome_fasta, mutation_metadata_df = mutation_metadata_df, read_df = read_df, n = number_of_random_reads_genome, read_length = read_length, input_type = \"genome\", strand = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kb count\n",
    "kb_count_command = [\"kb\", \"count\", \"-t\", str(threads), \"--mm\", \"-k\", str(k), \"-i\", mutation_index, \"-g\", mutation_t2g, \"-x\", \"bulk\", \"--num\", \"--h5ad\", \"--parity\", \"single\", \"-o\", kb_count_out_mm, synthetic_reads_fastQ]\n",
    "if not os.path.exists(kb_count_out_mm) or len(os.listdir(kb_count_out_mm)) == 0:\n",
    "    subprocess.run(kb_count_command, check=True)\n",
    "\n",
    "kb_count_command.remove(\"--mm\")\n",
    "kb_count_command[kb_count_command.index(kb_count_out_mm)] = kb_count_out  # swap kb_count_out_mm for kb_count_out\n",
    "if not os.path.exists(kb_count_out) or len(os.listdir(kb_count_out)) == 0:\n",
    "    subprocess.run(kb_count_command, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import varseek.utils.seq_utils\n",
    "importlib.reload(varseek.utils.seq_utils)\n",
    "from varseek.utils.seq_utils import make_bus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_df = make_bus_df(kallisto_out = kb_count_out_mm, fastq_file_list = synthetic_reads_fastQ, t2g_file = mutation_t2g, mm = False, union = False, assay = \"bulk\", bustools = \"/home/jrich/miniconda3/envs/varseek/lib/python3.10/site-packages/kb_python/bins/linux/bustools/bustools\")\n",
    "bus_df.rename(columns={\"fastq_header\": \"read_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "check_assertions = False\n",
    "!cat {kb_count_out}/run_info.json | grep -E '\"n_targets\"|\"n_processed\"|\"n_pseudoaligned\"|\"n_unique\"|\"p_pseudoaligned\"|\"p_unique\"'\n",
    "\n",
    "with open(f\"{kb_count_out}/run_info.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "total_reads = data.get(\"n_processed\", None)\n",
    "n_pseudoaligned = data.get(\"n_pseudoaligned\", None)\n",
    "n_unique = data.get(\"n_unique\", None)\n",
    "n_multimapped = n_pseudoaligned - n_unique\n",
    "\n",
    "if check_assertions:\n",
    "    assert total_reads == n_pseudoaligned, \"Total reads and pseudoaligned reads do not match\"\n",
    "    assert total_reads == n_unique, \"Total reads and unique reads do not match\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perspective of reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_ids = set(bus_df['read_id'])\n",
    "read_df['aligned_somewhere'] = read_df['read_id'].isin(aligned_ids)\n",
    "\n",
    "read_df['TP_crude'] = (read_df['mutant_read'] & read_df['aligned_somewhere'])\n",
    "read_df['FP_crude'] = (~read_df['mutant_read'] & read_df['aligned_somewhere'])\n",
    "read_df['FN_crude'] = (read_df['mutant_read'] & ~read_df['aligned_somewhere'])\n",
    "read_df['TN_crude'] = (~read_df['mutant_read'] & ~read_df['aligned_somewhere'])\n",
    "\n",
    "metric_dictionary_reads_crude = calculate_metrics(read_df, header_name = \"read_header\", check_assertions = check_assertions, crude = True, out = f\"{plot_output_folder}/reads_metrics_crude.txt\")\n",
    "draw_confusion_matrix(metric_dictionary_reads_crude)\n",
    "\n",
    "true_set_crude = set(read_df.loc[read_df['mutant_read'], 'read_header'])\n",
    "positive_set_crude = set(read_df.loc[read_df['aligned_somewhere'], 'read_header'])\n",
    "create_venn_diagram(true_set_crude, positive_set_crude, TN = metric_dictionary_reads_crude['TN'], mm = n_multimapped, out_path = f\"{plot_output_folder}/venn_diagram_read_crude.png\")\n",
    "\n",
    "create_stratified_metric_bar_plot(read_df, 'mcrs_mutation_type', 'accuracy', overall_metric = metric_dictionary_reads_crude['accuracy'], log_x_axis = False, display_numbers = True, crude = True, out_path = f\"{plot_output_folder}/accuracy_vs_mcrs_mutation_type_read_perspective_crude.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge read_df with bus_df on 'read_id' and 'fastq_header' to bring in 'transcript_ids_list'\n",
    "merged_df = read_df.merge(bus_df[['read_id', 'transcript_ids_list']], \n",
    "                          on='read_id', \n",
    "                          how='left')\n",
    "\n",
    "# Create 'multimapped' column as True if length of 'transcript_ids_list' > 1, else False\n",
    "read_df['multimapped'] = merged_df['transcript_ids_list'].apply(lambda x: len(x) > 1 if isinstance(x, list) else False)\n",
    "\n",
    "\n",
    "read_df = read_df.merge(bus_df[['read_id', 'transcript_names_final']], \n",
    "                        on='read_id', \n",
    "                        how='left').rename(columns={'transcript_names_final': 'mcrs_id_to_which_the_read_aligned'})\n",
    "\n",
    "# Convert lists to comma-separated strings, keeping NaN values as NaN\n",
    "read_df['mcrs_id_to_which_the_read_aligned'] = read_df['mcrs_id_to_which_the_read_aligned'].apply(\n",
    "    lambda x: ','.join(x) if isinstance(x, list) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_df['aligned_to_correct_mcrs'] = (read_df['mcrs_id'] == read_df['mcrs_id_to_which_the_read_aligned'])\n",
    "\n",
    "# Check if 'mcrs_id' is a substring of any item in 'mcrs_id_to_which_the_read_aligned'\n",
    "read_df['aligned_to_correct_mcrs_including_multimapped'] = read_df.apply(\n",
    "    lambda row: row['mcrs_id'] in row['mcrs_id_to_which_the_read_aligned'] \n",
    "                if isinstance(row['mcrs_id_to_which_the_read_aligned'], str) else False,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "read_df['TP'] = (read_df['mutant_read'] & read_df['aligned_to_correct_mcrs'])\n",
    "read_df['FP'] = (~read_df['mutant_read'] & read_df['aligned_to_correct_mcrs'])\n",
    "read_df['FN'] = (read_df['mutant_read'] & ~read_df['aligned_to_correct_mcrs'])\n",
    "read_df['TN'] = (~read_df['mutant_read'] & ~read_df['aligned_to_correct_mcrs'])\n",
    "\n",
    "metric_dictionary_reads = calculate_metrics(read_df, header_name = \"read_header\", check_assertions = check_assertions, out = f\"{plot_output_folder}/read_metrics.txt\")\n",
    "draw_confusion_matrix(metric_dictionary_reads)\n",
    "\n",
    "true_set = set(read_df.loc[read_df['mutant_read'], 'read_header'])\n",
    "positive_set = set(read_df.loc[read_df['aligned_to_correct_mcrs'], 'read_header'])\n",
    "create_venn_diagram(true_set, positive_set, TN = metric_dictionary_reads['TN'], mm = n_multimapped, out_path = f\"{plot_output_folder}/venn_diagram_read.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_df['reason_for_fn'] = np.nan\n",
    "\n",
    "if check_reason_for_read_fn:\n",
    "    # Loop through each row in read_df where 'FN' is True\n",
    "    for index, row in read_df.loc[read_df['FN']].iterrows():\n",
    "        reason_for_fn_sublist = []\n",
    "\n",
    "        # Get the mcrs_id from the current row in read_df\n",
    "        mcrs_id = row['mcrs_id']\n",
    "        \n",
    "        # Find the corresponding mcrs_sequence_length in mutation_metadata_df\n",
    "        matching_length = mutation_metadata_df.loc[\n",
    "            mutation_metadata_df['mcrs_id'] == mcrs_id, 'mcrs_sequence_length'\n",
    "        ]\n",
    "        \n",
    "        # Check if there's a match and if the length is less than 55\n",
    "        if not matching_length.empty and matching_length.iloc[0] < (2*k-1):\n",
    "            reason_for_fn_sublist.append(\"short_mcrs_id\")\n",
    "\n",
    "        if row['multimapped']:\n",
    "            reason_for_fn_sublist.append(\"multimapped\")\n",
    "        \n",
    "        dlist_status = mutation_metadata_df.loc[\n",
    "            mutation_metadata_df['mcrs_id'] == mcrs_id, 'dlist'\n",
    "        ]\n",
    "\n",
    "        if not dlist_status.empty and dlist_status.iloc[0] != \"none\":\n",
    "            reason_for_fn_sublist.append(\"dlisted\")\n",
    "\n",
    "        if reason_for_fn_sublist:\n",
    "            read_df.at[index, 'reason_for_fn'] = reason_for_fn_sublist  # might be cast as a string if length == 1 (especially for first row), but this doesn't really matter for me\n",
    "\n",
    "    num_nan_reasons = read_df.loc[read_df['FN'], 'reason_for_fn'].isna().sum()\n",
    "\n",
    "    print(f\"Number of FN reads with no reasons: {num_nan_reasons}\")\n",
    "\n",
    "    # mcrs_set = set(unique_mcrs_df.loc[unique_mcrs_df['FN'] == True, 'mcrs_header'])\n",
    "    # filtered_read_df = read_df[(read_df['FN']) & (~read_df['reference_header'].isin(mcrs_set))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique mcrs_id values from read_df where FN is True\n",
    "matching_mcrs_ids = read_df.loc[read_df['FN'], 'mcrs_id'].unique()\n",
    "\n",
    "# Filter mutation_metadata_df for rows with matching mcrs_id values and access mcrs_sequence_length\n",
    "matching_mcrs_sequence_lengths = mutation_metadata_df.loc[\n",
    "    mutation_metadata_df['mcrs_id'].isin(matching_mcrs_ids), 'mcrs_sequence_length'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perspective of reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_list = [\"header\",\"order\",\"seq_ID\",\"mutation\",\"mutation_type\",\"wt_sequence\",\"nucleotide_positions\",\"actual_mutation\",\"mutation_cds\",\"mutation_aa\",\"GENOMIC_MUTATION_ID\",\"chromosome\",\"strand\",\"mutation_genome\",\"gene_name\",\"mutation_id\",\"start_mutation_position\",\"end_mutation_position\",\"nucleotide_positions_cdna\",\"actual_mutation_cdna\",\"start_mutation_position_cdna\",\"end_mutation_position_cdna\",\"header_genome\",\"header_cdna\",\"header_cds\",\"nucleotide_positions_genome\",\"actual_mutation_genome\",\"start_mutation_position_genome\",\"end_mutation_position_genome\",\"distance_to_nearest_splice_junction\",\"is_near_splice_junction_10\",\"nearby_mutations\",\"nearby_mutations_count\",\"has_a_nearby_mutation\"]\n",
    "columns_to_or = [\"included_in_synthetic_reads_mutant\",\"included_in_synthetic_reads_wt\",\"included_in_synthetic_reads\", \"any_noisy_reads\", \"any_noisy_reads_mutant\", \"any_noisy_reads_wt\"]\n",
    "columns_to_sum = [\"list_of_read_starting_indices_mutant\",\"list_of_read_starting_indices_wt\",\"number_of_reads_wt\",\"number_of_reads_mutant\",\"noisy_read_indices_mutant\",\"noisy_read_indices_wt\"]\n",
    "columns_to_all = []\n",
    "\n",
    "# #!!!TEMP\n",
    "# drop all columns in columns_to_list\n",
    "columns_to_list.remove(\"order\")\n",
    "columns_to_list.remove(\"distance_to_nearest_splice_junction\")\n",
    "mutation_metadata_df.drop(columns=columns_to_list, inplace=True)\n",
    "# #!!!TEMP\n",
    "\n",
    "mutation_metadata_df_original_column_order = mutation_metadata_df.columns\n",
    "columns_to_list = [col for col in columns_to_list if col in mutation_metadata_df_original_column_order]\n",
    "columns_to_or = [col for col in columns_to_or if col in mutation_metadata_df_original_column_order]\n",
    "columns_to_sum = [col for col in columns_to_sum if col in mutation_metadata_df_original_column_order]\n",
    "\n",
    "for column in list(columns_to_list):\n",
    "    mutation_metadata_df[column] = mutation_metadata_df[column].apply(\n",
    "        lambda x: tuple(x) if isinstance(x, list) else x\n",
    "    )\n",
    "\n",
    "unique_mcrs_df = (\n",
    "    mutation_metadata_df.sort_values('order').groupby('mcrs_header', as_index=False)\n",
    "    .agg(\n",
    "        {**{col: list for col in list(columns_to_list)},  # list these values\n",
    "        **{col: sum for col in list(columns_to_sum)},  # Take the first value for these columns\n",
    "        **{col: np.any for col in list(columns_to_or)},  # OR these values\n",
    "        **{col: np.all for col in list(columns_to_all)},  # AND these values\n",
    "        **{col: 'first' for col in mutation_metadata_df.columns if col not in columns_to_list + columns_to_or + columns_to_all + columns_to_sum + ['mcrs_header']}}  # Take the first value for other columns\n",
    "    ).reset_index(drop=True)\n",
    ")\n",
    "\n",
    "unique_mcrs_df = unique_mcrs_df[mutation_metadata_df_original_column_order]\n",
    "\n",
    "unique_mcrs_df['tumor_purity'] = unique_mcrs_df['number_of_reads_mutant'] / unique_mcrs_df['number_of_reads_wt']\n",
    "\n",
    "unique_mcrs_df['tumor_purity'] = np.where(\n",
    "    np.isnan(unique_mcrs_df['tumor_purity']), \n",
    "    np.nan,  # Keep NaN as NaN\n",
    "    unique_mcrs_df['tumor_purity']  # Keep the result for valid divisions\n",
    ")\n",
    "\n",
    "try:\n",
    "    unique_mcrs_df['min_distance_to_splice_junction'] = unique_mcrs_df['distance_to_nearest_splice_junction'].apply(\n",
    "        lambda x: min(x) if isinstance(x, list) and len(x) > 0 else np.nan\n",
    "    )\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (very low priority): once I run more complex tests to verify that bus df and adata agree, then I can load in these values from bus df and remove the need to run kb count without mm\n",
    "\n",
    "adata_path = f\"{kb_count_out}/counts_unfiltered/adata.h5ad\"\n",
    "adata = ad.read_h5ad(adata_path)\n",
    "\n",
    "# Find the indices of non-zero values in adata.X\n",
    "non_zero_indices = np.nonzero(adata.X)\n",
    "\n",
    "# Get the non-zero values\n",
    "non_zero_values = np.squeeze(np.asarray(adata.X[non_zero_indices])).astype(int)\n",
    "\n",
    "# Get the corresponding gene names\n",
    "mcrs_ids = adata.var.index[non_zero_indices[1]].to_numpy().flatten()\n",
    "\n",
    "mcrs_ids = mcrs_ids.astype(str)\n",
    "\n",
    "# Create a DataFrame\n",
    "adata_df = pd.DataFrame({\n",
    "    'mcrs_id': mcrs_ids,\n",
    "    'number_of_reads_aligned_to_this_item': non_zero_values\n",
    "})\n",
    "\n",
    "unique_mcrs_df = unique_mcrs_df.merge(adata_df, on='mcrs_id', how='left')\n",
    "unique_mcrs_df['received_an_aligned_read'] = ~pd.isna(unique_mcrs_df['number_of_reads_aligned_to_this_item'])\n",
    "\n",
    "# # set to int - will need to do Int64 to work\n",
    "# unique_mcrs_df['number_of_reads_mutant'] = unique_mcrs_df['number_of_reads_mutant'].astype(int)\n",
    "# unique_mcrs_df['number_of_reads_wt'] = unique_mcrs_df['number_of_reads_wt'].astype(int)\n",
    "# unique_mcrs_df['number_of_reads_aligned_to_this_item'] = unique_mcrs_df['number_of_reads_aligned_to_this_item'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary detour: make sure bus_df and adata agree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_df_subset = bus_df[[\"transcript_names_final\", \"counted_in_count_matrix\", \"count\"]]\n",
    "\n",
    "bus_df_subset['transcript_names_final'] = bus_df_subset['transcript_names_final'].apply(\n",
    "    lambda x: x[0] if isinstance(x, list) and x else x\n",
    ")\n",
    "\n",
    "# Group by the string representation\n",
    "bus_df_subset_grouped = bus_df_subset.groupby('transcript_names_final', as_index=False).agg({\n",
    "    'counted_in_count_matrix': 'first',\n",
    "    'count': 'sum'\n",
    "})\n",
    "\n",
    "# Set values in 'count' to 0 where 'counted_in_count_matrix' is False\n",
    "bus_df_subset_grouped.loc[bus_df_subset_grouped['counted_in_count_matrix'] == False, 'count'] = 0\n",
    "\n",
    "bus_df_subset_grouped.rename(columns={\"transcript_names_final\": \"mcrs_id\", \"count\": \"count_bus\"}, inplace=True)\n",
    "\n",
    "bus_df_subset_grouped = bus_df_subset_grouped.merge(adata_df, on='mcrs_id', how='left')\n",
    "bus_df_subset_grouped = bus_df_subset_grouped.rename(columns={'number_of_reads_aligned_to_this_item': 'count_adata'})\n",
    "\n",
    "\n",
    "mismatch_count = (bus_df_subset_grouped['count_bus'] != bus_df_subset_grouped['count_adata']).sum()\n",
    "print(f\"Number of mismatches between BUS file and adata: {mismatch_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_mcrs_df['TP_crude'] = (unique_mcrs_df['included_in_synthetic_reads_mutant'] & unique_mcrs_df['received_an_aligned_read'])\n",
    "unique_mcrs_df['FP_crude'] = (~unique_mcrs_df['included_in_synthetic_reads_mutant'] & unique_mcrs_df['received_an_aligned_read'])\n",
    "unique_mcrs_df['FN_crude'] = (unique_mcrs_df['included_in_synthetic_reads_mutant'] & ~unique_mcrs_df['received_an_aligned_read'])\n",
    "unique_mcrs_df['TN_crude'] = (~unique_mcrs_df['included_in_synthetic_reads_mutant'] & ~unique_mcrs_df['received_an_aligned_read'])\n",
    "\n",
    "metric_dictionary_reference = calculate_metrics(unique_mcrs_df, header_name = \"mcrs_header\", check_assertions = check_assertions, crude = True, out = f\"{plot_output_folder}/reference_metrics_crude.txt\")\n",
    "draw_confusion_matrix(metric_dictionary_reference)\n",
    "\n",
    "true_set = set(unique_mcrs_df.loc[unique_mcrs_df['included_in_synthetic_reads_mutant'], 'mcrs_header'])\n",
    "positive_set = set(unique_mcrs_df.loc[unique_mcrs_df['received_an_aligned_read'], 'mcrs_header'])\n",
    "create_venn_diagram(true_set, positive_set, TN = metric_dictionary_reference['TN'], out_path = f\"{plot_output_folder}/venn_diagram_reference_crude.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_read_df_mutant = read_df.loc[(read_df['aligned_somewhere'] == True) & (read_df['mutant_read'] == True)]\n",
    "mcrs_counts_mutant = aligned_read_df_mutant.groupby('mcrs_id').size().reset_index(name='number_of_MUTANT_reads_belonging_to_the_mcrs_item_that_aligned_somewhere')\n",
    "\n",
    "unique_mcrs_df = unique_mcrs_df.merge(mcrs_counts_mutant, on='mcrs_id', how='left')\n",
    "unique_mcrs_df['number_of_MUTANT_reads_belonging_to_the_mcrs_item_that_aligned_somewhere'] = unique_mcrs_df['number_of_MUTANT_reads_belonging_to_the_mcrs_item_that_aligned_somewhere'].fillna(0).astype(int)\n",
    "unique_mcrs_df['number_of_MUTANT_reads_belonging_to_the_mcrs_item_that_didnt_align_anywhere'] = unique_mcrs_df['number_of_reads_mutant'] - unique_mcrs_df['number_of_MUTANT_reads_belonging_to_the_mcrs_item_that_aligned_somewhere']\n",
    "\n",
    "unique_mcrs_df['mutation_expression_prediction_error'] = unique_mcrs_df['number_of_reads_aligned_to_this_item'] - unique_mcrs_df['number_of_reads_mutant']  # positive means overpredicted, negative means underpredicted\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "aligned_read_df_wt = read_df.loc[(read_df['aligned_somewhere'] == True) & (read_df['wt_read'] == True)]\n",
    "mcrs_counts_wt = aligned_read_df_wt.groupby('mcrs_id').size().reset_index(name='number_of_WT_reads_belonging_to_the_mcrs_item_that_aligned_somewhere')\n",
    "\n",
    "unique_mcrs_df = unique_mcrs_df.merge(mcrs_counts_wt, on='mcrs_id', how='left')\n",
    "unique_mcrs_df['number_of_WT_reads_belonging_to_the_mcrs_item_that_aligned_somewhere'] = unique_mcrs_df['number_of_WT_reads_belonging_to_the_mcrs_item_that_aligned_somewhere'].fillna(0).astype(int)\n",
    "unique_mcrs_df['number_of_WT_reads_belonging_to_the_mcrs_item_that_didnt_align_anywhere'] = unique_mcrs_df['number_of_reads_wt'] - unique_mcrs_df['number_of_WT_reads_belonging_to_the_mcrs_item_that_aligned_somewhere']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by 'mcrs_header' and count 'aligned_to_correct_mcrs' (sum will count True as 1, False as 0)\n",
    "aligned_counts = read_df.groupby('mcrs_header')['aligned_to_correct_mcrs'].sum().reset_index()\n",
    "\n",
    "# Rename the column to something meaningful, like 'correct_alignment_count'\n",
    "aligned_counts.rename(columns={'aligned_to_correct_mcrs': 'number_of_reads_belonging_to_this_mcrs_item_that_mapped_here_correctly'}, inplace=True)\n",
    "\n",
    "# Step 2: Merge the counts with unique_mcrs_df on the 'mcrs_header' column\n",
    "unique_mcrs_df = pd.merge(unique_mcrs_df, aligned_counts, on='mcrs_header', how='left')\n",
    "\n",
    "# Fill NaN values with 0 in case some mcrs_header values in unique_mcrs_df do not appear in read_df\n",
    "unique_mcrs_df['number_of_reads_belonging_to_this_mcrs_item_that_mapped_here_correctly'].fillna(0, inplace=True)\n",
    "\n",
    "unique_mcrs_df['received_an_aligned_read_from_one_of_its_corresponding_reads'] = unique_mcrs_df['number_of_reads_belonging_to_this_mcrs_item_that_mapped_here_correctly'] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by 'mcrs_header_to_which_the_read_aligned' and collect 'read_header' into lists\n",
    "reads_mapped = read_df.groupby('mcrs_id_to_which_the_read_aligned')['read_header'].apply(list).reset_index()\n",
    "\n",
    "# Rename the column to 'reads_mapped_to_this_reference_item' for clarity\n",
    "reads_mapped.rename(columns={'read_header': 'reads_mapped_to_this_reference_item'}, inplace=True)\n",
    "\n",
    "# Step 2: Merge the list of read headers with unique_mcrs_df on the 'mcrs_header' column\n",
    "unique_mcrs_df = pd.merge(unique_mcrs_df, reads_mapped, left_on='mcrs_id', right_on='mcrs_id_to_which_the_read_aligned', how='left')\n",
    "unique_mcrs_df.drop(columns='mcrs_id_to_which_the_read_aligned', inplace=True)\n",
    "\n",
    "# Fill NaN values with empty lists in case there are no matching read headers for some mcrs_header values\n",
    "unique_mcrs_df['reads_mapped_to_this_reference_item'] = unique_mcrs_df['reads_mapped_to_this_reference_item'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "unique_mcrs_df['number_of_MUTANT_reads_belonging_to_the_mcrs_item_that_aligned_somewhere'] = unique_mcrs_df['number_of_MUTANT_reads_belonging_to_the_mcrs_item_that_aligned_somewhere'].astype(bool)\n",
    "unique_mcrs_df['number_of_WT_reads_belonging_to_the_mcrs_item_that_aligned_somewhere'] = unique_mcrs_df['number_of_WT_reads_belonging_to_the_mcrs_item_that_aligned_somewhere'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_mcrs_df['TP'] = (unique_mcrs_df['included_in_synthetic_reads_mutant'] & unique_mcrs_df['received_an_aligned_read_from_one_of_its_corresponding_reads'])\n",
    "unique_mcrs_df['FP'] = (~unique_mcrs_df['included_in_synthetic_reads_mutant'] & unique_mcrs_df['received_an_aligned_read_from_one_of_its_corresponding_reads'])\n",
    "unique_mcrs_df['FN'] = (unique_mcrs_df['included_in_synthetic_reads_mutant'] & ~unique_mcrs_df['received_an_aligned_read_from_one_of_its_corresponding_reads'])\n",
    "unique_mcrs_df['TN'] = (~unique_mcrs_df['included_in_synthetic_reads_mutant'] & ~unique_mcrs_df['received_an_aligned_read_from_one_of_its_corresponding_reads'])\n",
    "\n",
    "metric_dictionary_reference = calculate_metrics(unique_mcrs_df, header_name = \"mcrs_header\", check_assertions = check_assertions, out = f\"{plot_output_folder}/reference_metrics.txt\")\n",
    "draw_confusion_matrix(metric_dictionary_reference)\n",
    "\n",
    "true_set = set(unique_mcrs_df.loc[unique_mcrs_df['included_in_synthetic_reads_mutant'], 'mcrs_header'])\n",
    "positive_set = set(unique_mcrs_df.loc[unique_mcrs_df['received_an_aligned_read_from_one_of_its_corresponding_reads'], 'mcrs_header'])\n",
    "create_venn_diagram(true_set, positive_set, TN = metric_dictionary_reference['TN'], out_path = f\"{plot_output_folder}/venn_diagram_reference.png\")\n",
    "\n",
    "unique_mcrs_df['number_of_reads_belonging_to_this_mcrs_item_that_did_not_align_correctly'] = unique_mcrs_df['number_of_reads_mutant'] - unique_mcrs_df['number_of_reads_belonging_to_this_mcrs_item_that_mapped_here_correctly']\n",
    "unique_mcrs_df['number_of_reads_aligned_to_this_mcrs_item_that_aligned_incorrectly'] = unique_mcrs_df['number_of_reads_aligned_to_this_item'] - unique_mcrs_df['number_of_reads_belonging_to_this_mcrs_item_that_mapped_here_correctly']\n",
    "\n",
    "plot_histogram(unique_mcrs_df.loc[~unique_mcrs_df['TN']], 'mutation_expression_prediction_error', log_scale = False, out_path = f\"{plot_output_folder}/histogram_mutation_expression_prediction_error_linear_axis.png\")\n",
    "plot_histogram(unique_mcrs_df.loc[~unique_mcrs_df['TN']], 'mutation_expression_prediction_error', log_scale = True, out_path = f\"{plot_output_folder}/histogram_mutation_expression_prediction_error_log_axis.png\")\n",
    "\n",
    "metric_dictionary_reference['mutation_expression_prediction_error_abs_mean'] = unique_mcrs_df.loc[~unique_mcrs_df['TN'], 'mutation_expression_prediction_error'].abs().mean()\n",
    "create_stratified_metric_bar_plot(unique_mcrs_df, 'number_of_reads_mutant', 'expression_error', overall_metric = metric_dictionary_reference['mutation_expression_prediction_error_abs_mean'], log_x_axis = False, out_path = f\"{plot_output_folder}/expression_error_vs_number_of_reads_mutant.png\")\n",
    "# create_stratified_metric_bar_plot(unique_mcrs_df, 'min_distance_to_splice_junction', 'accuracy', overall_metric = metric_dictionary_reference['accuracy'], log_x_axis = False, bins = [-1, 0, 1, 5, 10, 50, 100, float('inf')])\n",
    "create_stratified_metric_bar_plot(unique_mcrs_df, 'number_of_reads_mutant', 'sensitivity', overall_metric = metric_dictionary_reference['sensitivity'], log_x_axis = False, out_path = f\"{plot_output_folder}/accuracy_vs_number_of_reads_mutant.png\")\n",
    "#* add more here\n",
    "#*** create similar plots for y in {sensitivity, specificity}, and x in {number_of_reads_wt, tumor_purity} and determine cutoffs for which varseek is reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexplainable_references = 0\n",
    "for header in metric_dictionary_reference['FNs']:\n",
    "    print(f\"Header {header}\")\n",
    "    did_not_align_anywhere = (read_df[read_df['mcrs_header'] == header])['FN_crude'].all()\n",
    "    print(f\"All did not align anywhere for {header}: {did_not_align_anywhere}\")\n",
    "    did_not_align_to_correct_position = (read_df[read_df['mcrs_header'] == header])['FN'].all()\n",
    "    print(f\"All did not align to correct mcrs for {header}: {did_not_align_to_correct_position}\")\n",
    "    multimapped = (read_df[read_df['mcrs_header'] == header])['multimapped'].all()\n",
    "    print(f\"All multimapped for {header}: {multimapped}\")\n",
    "    if not did_not_align_anywhere and not did_not_align_to_correct_position and not multimapped:\n",
    "        unexplainable_references += 1\n",
    "\n",
    "if unexplainable_references > 0:\n",
    "    print(f\"Unexplainable references: {unexplainable_references}\")\n",
    "else:\n",
    "    print(\"All explainable!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_mcrs_df.to_csv(unique_mcrs_df_out, index=False)\n",
    "read_df.to_csv(read_df_out_updated, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_df = pd.read_csv(read_df_out_updated)\n",
    "# unique_mcrs_df = pd.read_csv(unique_mcrs_df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read_df.columns)\n",
    "print(unique_mcrs_df.columns)\n",
    "print(mutation_metadata_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eg /home/jrich/data/varseek_data_fresh/vk_sim_2024nov24_temp/plots/read_metrics.txt or /home/jrich/data/varseek_data_fresh/vk_sim_2024nov24_temp/plots/reference_metrics.txt\n",
    "# no_noise_read_metrics = \"\"\n",
    "# substitution_noise_read_metrics = \"\"\n",
    "# deletion_noise_read_metrics = \"\"\n",
    "# insertion_noise_read_metrics = \"\"\n",
    "# all_noise_read_metrics = \"\"\n",
    "# metric = \"accuracy\"\n",
    "\n",
    "# from varseek.utils import retrieve_value_from_metric_file\n",
    "\n",
    "# accuracy_dict = {}\n",
    "# accuracy_dict['No Noise'] = retrieve_value_from_metric_file(metric, no_noise_read_metrics)\n",
    "# accuracy_dict['Substitution Noise'] = retrieve_value_from_metric_file(metric, substitution_noise_read_metrics)\n",
    "# accuracy_dict['Deletion Noise'] = retrieve_value_from_metric_file(metric, deletion_noise_read_metrics)\n",
    "# accuracy_dict['Insertion Noise'] = retrieve_value_from_metric_file(metric, insertion_noise_read_metrics)\n",
    "# accuracy_dict['All Noise'] = retrieve_value_from_metric_file(metric, all_noise_read_metrics)\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# plot_basic_bar_plot_from_dict(accuracy_dict, y_axis=metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating with kb extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kb_extract_out_all_slow = f\"{out_dir_notebook}/kb_extract_out_all_slow\"\n",
    "# df_extract_combined = pd.DataFrame(columns=['read_id', 'mcrs_id_to_which_the_read_aligned_kb_extract'])\n",
    "\n",
    "# if strand is None or strand == \"both\":\n",
    "#     strand_kb_extract = \"unstranded\"\n",
    "# elif strand == \"f\":\n",
    "#     strand_kb_extract = \"forward\"\n",
    "# elif strand == \"r\":\n",
    "#     strand_kb_extract = \"reverse\"\n",
    "\n",
    "# # # running with --extract_all will make all ~5.3M reference items as the targets, which is much longer than manually defining the list of mutations as my targets - but I would imagine the outcome is the same; also, extract_all will output all files at once rather than one at a time (not good if I want to check progress throughout or break up for multithreading, but it can avoid the issue where mcrs_string_with_aligned_reads is too many characters)\n",
    "# # kb_extract_command = f\"kb extract --strand {strand_kb_extract} --verbose -k {k} -t {threads} --extract_all -o {kb_extract_out_all_slow} -i {mutation_index} -g {mutation_t2g} {synthetic_reads_fastQ}\"\n",
    "# adata_path = f\"{kb_count_out}/counts_unfiltered/adata.h5ad\"\n",
    "# mapped_mutations_string = find_genes_with_aligned_reads_for_kb_extract(adata_path)\n",
    "# mcrs_set_with_aligned_reads = set(mapped_mutations_string.split())\n",
    "# mcrs_string_with_aligned_reads = ' '.join(f\"'{x}'\" for x in mcrs_set_with_aligned_reads)\n",
    "# kb_extract_command = f\"kb extract --verbose -k {k} --mm -t {threads} --targets {mcrs_string_with_aligned_reads} -o {kb_extract_out_all_slow} -i {mutation_index} -g {mutation_t2g} {synthetic_reads_fastQ}\"\n",
    "\n",
    "# print(kb_extract_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the command in tmux!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'bus_df' not in globals():\n",
    "#     from varseek.utils import make_bus_df\n",
    "#     bus_df = make_bus_df(kallisto_out = kb_count_out_mm, fastq_file_list = synthetic_reads_fastQ, t2g_file = mutation_t2g, mm = True, union = False, assay = \"bulk\", bustools = \"/home/jrich/miniconda3/envs/varseek/lib/python3.10/site-packages/kb_python/bins/linux/bustools/bustools\")\n",
    "#     bus_df.rename(columns={\"fastq_header\": \"read_id\"}, inplace=True)\n",
    "\n",
    "# # add some code here to check where these map\n",
    "# for subdir in os.listdir(kb_extract_out_all_slow):\n",
    "#     subdir_path = os.path.join(kb_extract_out_all_slow, subdir)\n",
    "#     if os.path.isdir(subdir_path):\n",
    "#         fastq_file = os.path.join(subdir_path, '1.fastq.gz')\n",
    "#         if os.path.exists(fastq_file):\n",
    "#             # Run get_header_set_from_fastq for the fastq file\n",
    "#             aligned_reads_kb_extract = get_header_set_from_fastq(fastq_file)\n",
    "#             df_temp = pd.DataFrame(aligned_reads_kb_extract, columns=['read_id'])\n",
    "#             df_temp['mcrs_id_to_which_the_read_aligned_kb_extract'] = subdir  # Add subdirectory info\n",
    "#             df_extract_combined = pd.concat([df_extract_combined, df_temp], ignore_index=True)  # Concatenate the new DataFrame with df_extract_combined\n",
    "\n",
    "# df_extract_combined = df_extract_combined.groupby('read_id')['mcrs_id_to_which_the_read_aligned_kb_extract'].apply(list).reset_index()\n",
    "\n",
    "# df_extract_combined = df_extract_combined.merge(bus_df[[\"read_id\", \"gene_names_final\"]], on='read_id', how='left')\n",
    "# df_extract_combined.rename(columns={\"gene_names_final\": \"mcrs_id_to_which_the_read_aligned_bus_df\"}, inplace=True)\n",
    "# mismatch_count_kb_extract = (df_extract_combined['mcrs_id_to_which_the_read_aligned_bus_df'] != df_extract_combined['mcrs_id_to_which_the_read_aligned_kb_extract']).sum()\n",
    "# print(f\"Number of mismatches between BUS file and kb extract: {mismatch_count_kb_extract}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kvar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
