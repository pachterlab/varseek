{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow followed according to: https://gatk.broadinstitute.org/hc/en-us/articles/360035531192-RNAseq-short-variant-discovery-SNPs-Indels\n",
    "\n",
    "Github workflow for GATK4 here: https://github.com/gatk-workflows/gatk4-rnaseq-germline-snps-indels/blob/master/gatk4-rna-best-practices.wdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pysam\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from varseek.utils import convert_chromosome_value_to_int_when_possible, calculate_metrics, draw_confusion_matrix, create_venn_diagram, calculate_sensitivity_specificity, create_stratified_metric_bar_plot, add_vcf_info_to_cosmic_tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_fastq(output_path, num_sequences=5000, seq_length=150, quality_score=\"I\"):\n",
    "    if not os.path.exists(output_path):\n",
    "        if not os.path.exists(os.path.dirname(output_path)):\n",
    "            os.makedirs(os.path.dirname(output_path))\n",
    "        bases = ['A', 'C', 'G', 'T']\n",
    "        with open(output_path, \"w\") as fastq_file:\n",
    "            for i in range(num_sequences):\n",
    "                # Generate a random sequence of the specified length\n",
    "                sequence = ''.join(random.choices(bases, k=seq_length))\n",
    "                \n",
    "                # Create a quality string of the same length, filled with the specified quality score\n",
    "                quality = quality_score * seq_length\n",
    "                \n",
    "                # Write the FASTQ entry\n",
    "                fastq_file.write(f\"@seq_{i + 1}\\n\")\n",
    "                fastq_file.write(f\"{sequence}\\n\")\n",
    "                fastq_file.write(\"+\\n\")\n",
    "                fastq_file.write(f\"{quality}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_read_fastq = \"/home/jrich/data/varseek_data_fresh/vk_sim_2024nov24_200mcrs_k59_nov16/synthetic_reads.fq\"  #!!! update path\n",
    "unique_mcrs_df_path = \"/home/jrich/data/varseek_data_fresh/vk_sim_2024nov24_200mcrs_k59_nov16/unique_mcrs_df.csv\"  #!!! update path\n",
    "gatk_parent = \"/home/jrich/data/varseek_data/gatk_nov17\"  #!!! update for each run\n",
    "# generate_random_fastq(synthetic_read_fastq)\n",
    "\n",
    "threads = 32\n",
    "read_length = 150\n",
    "mutation_source = \"cdna\"  # \"cdna\", \"cds\"\n",
    "\n",
    "cosmic_tsv = \"/home/jrich/data/varseek_data/reference/cosmic/CancerMutationCensus_AllData_Tsv_v100_GRCh37/CancerMutationCensus_AllData_v100_GRCh37.tsv\"\n",
    "cosmic_cdna_info_csv = \"/home/jrich/data/varseek_data/reference/cosmic/CancerMutationCensus_AllData_Tsv_v100_GRCh37/CancerMutationCensus_AllData_v100_GRCh37_mutation_workflow_with_cdna.csv\"\n",
    "\n",
    "# if these paths don't exist then they will be created\n",
    "reference_genome_fasta = \"/home/jrich/data/varseek_data/reference/ensembl_grch37_release93/Homo_sapiens.GRCh37.dna.primary_assembly.fa\"\n",
    "reference_genome_gtf = \"/home/jrich/data/varseek_data/reference/ensembl_grch37_release93/Homo_sapiens.GRCh37.87.gtf\"\n",
    "genomes1000_vcf = \"/home/jrich/data/varseek_data/reference/ensembl_grch37_release93/1000GENOMES-phase_3.vcf\"\n",
    "ensembl_germline_vcf = \"/home/jrich/data/varseek_data/reference/ensembl_grch37_release93/homo_sapiens.vcf\"\n",
    "star_genome_dir = \"/home/jrich/data/varseek_data/reference/ensembl_grch37_release93/star_reference\"\n",
    "\n",
    "STAR = \"/home/jrich/opt/STAR-2.7.11b/source/STAR\"\n",
    "java = \"/home/jrich/opt/java/jdk-17.0.12+7/bin/java\"\n",
    "picard_jar = \"/home/jrich/opt/picard/build/libs/picard.jar\"\n",
    "gatk = \"/home/jrich/opt/gatk-4.6.0.0/gatk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_home = os.path.dirname(os.path.dirname(java))\n",
    "\n",
    "os.environ['JAVA_HOME'] = java_home\n",
    "os.environ['PATH'] = f\"{os.environ['JAVA_HOME']}/bin:\" + os.environ['PATH']\n",
    "\n",
    "os.makedirs(star_genome_dir, exist_ok=True)\n",
    "\n",
    "alignment_folder = f\"{gatk_parent}/alignment\"\n",
    "os.makedirs(alignment_folder, exist_ok=True)\n",
    "\n",
    "gatk_supporting_files = f\"{gatk_parent}/supporting_files\"\n",
    "os.makedirs(gatk_supporting_files, exist_ok=True)\n",
    "\n",
    "ensembl_germline_vcf_filtered = ensembl_germline_vcf.replace(\".vcf\", \"_filtered.vcf\")\n",
    "\n",
    "out_file_name_prefix = f\"{alignment_folder}/sample_\"\n",
    "\n",
    "vcf_folder = f\"{gatk_parent}/vcfs\"\n",
    "haplotypecaller_folder = f\"{vcf_folder}/haplotypecaller\"\n",
    "mutect2_folder = f\"{vcf_folder}/mutect2\"\n",
    "\n",
    "os.makedirs(vcf_folder, exist_ok=True)\n",
    "os.makedirs(haplotypecaller_folder, exist_ok=True)\n",
    "os.makedirs(mutect2_folder, exist_ok=True)\n",
    "\n",
    "aligned_and_unmapped_bam = f\"{out_file_name_prefix}Aligned.sortedByCoord.out.bam\"\n",
    "aligned_only_bam = f\"{alignment_folder}/aligned_only.bam\"\n",
    "unmapped_bam = f\"{alignment_folder}/unmapped.bam\"\n",
    "merged_bam = f\"{alignment_folder}/merged.bam\"\n",
    "\n",
    "marked_duplicates_bam = f\"{alignment_folder}/marked_duplicates.bam\"\n",
    "marked_dup_metrics_txt = f\"{alignment_folder}/marked_dup_metrics.txt\"\n",
    "\n",
    "split_n_cigar_reads_bam = f\"{alignment_folder}/split_n_cigar_reads.bam\"\n",
    "recal_data_table = f\"{alignment_folder}/recal_data.table\"\n",
    "recalibrated_bam = f\"{alignment_folder}/recalibrated.bam\"\n",
    "covariates_plot = f\"{alignment_folder}/AnalyzeCovariates.pdf\"\n",
    "haplotypecaller_unfiltered_vcf = f\"{haplotypecaller_folder}/haplotypecaller_output_unfiltered.g.vcf.gz\"\n",
    "\n",
    "haplotypecaller_filtered_vcf = f\"{haplotypecaller_folder}/haplotypecaller_output_filtered.vcf.gz\"\n",
    "haplotypecaller_filtered_applied_vcf = f\"{haplotypecaller_folder}/haplotypecaller_output_filtered_applied.vcf.gz\"\n",
    "\n",
    "panel_of_normals_vcf = f\"{gatk_supporting_files}/1000g_pon.hg38.vcf.gz\"\n",
    "panel_of_normals_vcf_filtered = f\"{gatk_supporting_files}/1000g_pon.hg38_filtered.vcf.gz\"\n",
    "mutect2_unfiltered_vcf = f\"{mutect2_folder}/mutect2_output_unfiltered.g.vcf.gz\"\n",
    "mutect2_filtered_vcf = f\"{mutect2_folder}/mutect2_output_filtered.vcf.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(reference_genome_fasta):\n",
    "    !wget -O {reference_genome_fasta}.gz https://ftp.ensembl.org/pub/grch37/release-93/fasta/homo_sapiens/dna/Homo_sapiens.GRCh37.dna.primary_assembly.fa.gz && gunzip {reference_genome_fasta}.gz\n",
    "\n",
    "if not os.path.exists(reference_genome_gtf):\n",
    "    !wget -O {reference_genome_gtf}.gz https://ftp.ensembl.org/pub/grch37/release-93/gtf/homo_sapiens/Homo_sapiens.GRCh37.87.gtf.gz && gunzip {reference_genome_gtf}.gz\n",
    "\n",
    "if not os.path.exists(genomes1000_vcf):\n",
    "    !wget -O {genomes1000_vcf}.gz https://ftp.ensembl.org/pub/grch37/release-93/variation/vcf/homo_sapiens/1000GENOMES-phase_3.vcf.gz && gunzip {genomes1000_vcf}.gz\n",
    "    \n",
    "if not os.path.exists(ensembl_germline_vcf):\n",
    "    !wget -O {ensembl_germline_vcf}.gz https://ftp.ensembl.org/pub/grch37/release-93/variation/vcf/homo_sapiens/homo_sapiens.vcf.gz && gunzip {ensembl_germline_vcf}.gz\n",
    "\n",
    "if not os.path.exists(ensembl_germline_vcf_filtered):\n",
    "    ensembl_germline_vcf_temp1 = ensembl_germline_vcf.replace(\".vcf\", \"_temp1.vcf\")\n",
    "    # ensembl_germline_vcf_temp2 = ensembl_germline_vcf.replace(\".vcf\", \"_temp2.vcf\")\n",
    "    !grep -vP '^[^\\t]*\\t[^\\t]*\\t[^\\t]*\\t[^\\t]*[WH]|^[^\\t]*\\t[^\\t]*\\t[^\\t]*\\t[^\\t]*\\t[^\\t]*[WH]' $ensembl_germline_vcf > $ensembl_germline_vcf_temp1\n",
    "    !grep -vP '^[^\\t]*\\t[^\\t]*\\t[^\\t]*\\t\\t|^[^\\t]*\\t[^\\t]*\\t[^\\t]*\\t[^\\t]*\\t\\t' $ensembl_germline_vcf_temp1 > $ensembl_germline_vcf_filtered\n",
    "    !rm $ensembl_germline_vcf_temp1\n",
    "\n",
    "# if not os.path.exists(panel_of_normals_vcf):\n",
    "#     !wget -P {gatk_supporting_files} https://storage.googleapis.com/gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz\n",
    "#     !wget -P {gatk_supporting_files} https://storage.googleapis.com/gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz.tbi\n",
    "\n",
    "# if not os.path.exists(panel_of_normals_vcf_filtered):\n",
    "#     decompressed_pon_vcf = panel_of_normals_vcf.replace(\".vcf.gz\", \".vcf\")\n",
    "#     !gunzip -c $panel_of_normals_vcf > $decompressed_pon_vcf\n",
    "    \n",
    "#     # Remove 'chr' prefix from data lines\n",
    "#     !sed -i 's/^chr//' $decompressed_pon_vcf\n",
    "\n",
    "#     # Remove 'chr' prefix from contig headers in the VCF\n",
    "#     !sed -i '/^##contig/s/ID=chr/ID=/' $decompressed_pon_vcf\n",
    "\n",
    "#     # gunzip again\n",
    "#     !bgzip -c $decompressed_pon_vcf > $panel_of_normals_vcf_filtered  # TODO: get bgzip installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Mapping to the Reference with STAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_length_minus_one = read_length - 1\n",
    "\n",
    "if not os.listdir(star_genome_dir):\n",
    "    !$STAR \\\n",
    "        --runThreadN $threads \\\n",
    "        --runMode genomeGenerate \\\n",
    "        --genomeDir $star_genome_dir \\\n",
    "        --genomeFastaFiles $reference_genome_fasta \\\n",
    "        --sjdbGTFfile $reference_genome_gtf \\\n",
    "        --sjdbOverhang $read_length_minus_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t/home/jrich/opt/STAR-2.7.11b/source/STAR --runThreadN 32 --genomeDir /home/jrich/data/varseek_data/reference/ensembl_grch37_release93/star_reference --readFilesIn /home/jrich/data/varseek_data_fresh/vk_sim_2024nov24_200mcrs_k59_nov16/synthetic_reads.fq --sjdbOverhang 149 --outFileNamePrefix /home/jrich/data/varseek_data/gatk_nov17/alignment/sample_ --outSAMtype BAM SortedByCoordinate --outSAMunmapped Within --outSAMmapqUnique 60 --twopassMode Basic\n",
      "\tSTAR version: 2.7.11b   compiled: 2024-07-22T09:22:47-0700 dator:/home/jrich/opt/STAR-2.7.11b/source\n",
      "Nov 20 16:11:16 ..... started STAR run\n",
      "Nov 20 16:11:16 ..... loading genome\n",
      "Nov 20 16:11:57 ..... started 1st pass mapping\n",
      "Nov 20 16:12:00 ..... finished 1st pass mapping\n",
      "Nov 20 16:12:01 ..... inserting junctions into the genome indices\n",
      "Nov 20 16:13:32 ..... started mapping\n",
      "Nov 20 16:13:36 ..... finished mapping\n",
      "Nov 20 16:13:37 ..... started sorting BAM\n",
      "Nov 20 16:13:38 ..... finished successfully\n"
     ]
    }
   ],
   "source": [
    "#* --outSAMmapqUnique 60 - change from default of 255 to avoid colliding with some aligners' use of 255 as a special value\n",
    "\n",
    "!$STAR \\\n",
    "    --runThreadN $threads \\\n",
    "    --genomeDir $star_genome_dir \\\n",
    "    --readFilesIn $synthetic_read_fastq \\\n",
    "    --sjdbOverhang $read_length_minus_one \\\n",
    "    --outFileNamePrefix $out_file_name_prefix \\\n",
    "    --outSAMtype BAM SortedByCoordinate \\\n",
    "    --outSAMunmapped Within \\\n",
    "    --outSAMmapqUnique 60 \\\n",
    "    --twopassMode Basic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate unmapped reads into its own BAM file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pysam.AlignmentFile(aligned_and_unmapped_bam, \"rb\") as bam_in: \n",
    "#     # unmapped_header = bam_in.header.to_dict()\n",
    "#     # if 'PG' in unmapped_header:\n",
    "#     #     del unmapped_header['PG']\n",
    "           \n",
    "#     with pysam.AlignmentFile(aligned_only_bam, \"wb\", template=bam_in) as bam_aligned_out, pysam.AlignmentFile(unmapped_bam, \"wb\", template=bam_in) as bam_unmapped_out:\n",
    "#         for read in bam_in:\n",
    "#             if read.is_unmapped:\n",
    "#                 bam_unmapped_out.write(read)  # Write unmapped read to the unmapped BAM file\n",
    "#             else:\n",
    "#                 bam_aligned_out.write(read)  # Write aligned read to the aligned BAM file\n",
    "\n",
    "# print(f\"Unmapped reads written to {unmapped_bam}\")\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:13:40.706 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/jrich/opt/picard/build/libs/picard.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "[Wed Nov 20 16:13:40 PST 2024] FastqToSam --FASTQ /home/jrich/data/varseek_data_fresh/vk_sim_2024nov24_200mcrs_k59_nov16/synthetic_reads.fq --OUTPUT /home/jrich/data/varseek_data/gatk_nov17/alignment/unmapped.bam --READ_GROUP_NAME rg1 --SAMPLE_NAME sample1 --LIBRARY_NAME lib1 --PLATFORM_UNIT unit1 --PLATFORM ILLUMINA --SEQUENCING_CENTER center1 --USE_SEQUENTIAL_FASTQS false --SORT_ORDER queryname --MIN_Q 0 --MAX_Q 93 --STRIP_UNPAIRED_MATE_NUMBER false --ALLOW_AND_IGNORE_EMPTY_LINES false --ALLOW_EMPTY_FASTQ false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 5 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false\n",
      "[Wed Nov 20 16:13:40 PST 2024] Executing as jrich@dator on Linux 3.10.0-1127.13.1.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 17.0.12+7; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:3.2.0-4-ge0475183b-SNAPSHOT\n",
      "WARNING\t2024-11-20 16:13:41\tFastqToSam\tMaking ambiguous determination about fastq's quality encoding; more than one format possible based on observed qualities.\n",
      "INFO\t2024-11-20 16:13:41\tFastqToSam\tAuto-detected quality format as: Illumina.\n",
      "INFO\t2024-11-20 16:13:41\tFastqToSam\tProcessed 56616 fastq reads\n",
      "[Wed Nov 20 16:13:41 PST 2024] picard.sam.FastqToSam done. Elapsed time: 0.02 minutes.\n",
      "Runtime.totalMemory()=2181038080\n"
     ]
    }
   ],
   "source": [
    "!$java -jar $picard_jar FastqToSam \\\n",
    "    -FASTQ $synthetic_read_fastq \\\n",
    "    -OUTPUT $unmapped_bam \\\n",
    "    -READ_GROUP_NAME rg1 \\\n",
    "    -SAMPLE_NAME sample1 \\\n",
    "    -LIBRARY_NAME lib1 \\\n",
    "    -PLATFORM_UNIT unit1 \\\n",
    "    -PLATFORM ILLUMINA \\\n",
    "    -SEQUENCING_CENTER center1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. MergeBamAlignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:13:42.612 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/jrich/opt/picard/build/libs/picard.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "[Wed Nov 20 16:13:42 PST 2024] CreateSequenceDictionary --OUTPUT /home/jrich/data/varseek_data/reference/ensembl_grch37_release93/Homo_sapiens.GRCh37.dna.primary_assembly.dict --REFERENCE /home/jrich/data/varseek_data/reference/ensembl_grch37_release93/Homo_sapiens.GRCh37.dna.primary_assembly.fa --TRUNCATE_NAMES_AT_WHITESPACE true --NUM_SEQUENCES 2147483647 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 5 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false\n",
      "[Wed Nov 20 16:13:42 PST 2024] Executing as jrich@dator on Linux 3.10.0-1127.13.1.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 17.0.12+7; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:3.2.0-4-ge0475183b-SNAPSHOT\n",
      "[Wed Nov 20 16:13:42 PST 2024] picard.sam.CreateSequenceDictionary done. Elapsed time: 0.00 minutes.\n",
      "Runtime.totalMemory()=2181038080\n",
      "To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp\n",
      "Exception in thread \"main\" picard.PicardException: file:///home/jrich/data/varseek_data/reference/ensembl_grch37_release93/Homo_sapiens.GRCh37.dna.primary_assembly.dict already exists.  Delete this file and try again, or specify a different output file.\n",
      "\tat picard.sam.CreateSequenceDictionary.doWork(CreateSequenceDictionary.java:227)\n",
      "\tat picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:281)\n",
      "\tat picard.cmdline.PicardCommandLine.instanceMain(PicardCommandLine.java:105)\n",
      "\tat picard.cmdline.PicardCommandLine.main(PicardCommandLine.java:115)\n"
     ]
    }
   ],
   "source": [
    "reference_genome_dict = reference_genome_fasta.replace(\".fa\", \".dict\")\n",
    "\n",
    "!$java -jar $picard_jar CreateSequenceDictionary \\\n",
    "    -R $reference_genome_fasta \\\n",
    "    -O $reference_genome_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:13:43.627 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/jrich/opt/picard/build/libs/picard.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "[Wed Nov 20 16:13:43 PST 2024] MergeBamAlignment --UNMAPPED_BAM /home/jrich/data/varseek_data/gatk_nov17/alignment/unmapped.bam --ALIGNED_BAM /home/jrich/data/varseek_data/gatk_nov17/alignment/sample_Aligned.sortedByCoord.out.bam --OUTPUT /home/jrich/data/varseek_data/gatk_nov17/alignment/merged.bam --SORT_ORDER coordinate --INCLUDE_SECONDARY_ALIGNMENTS false --VALIDATION_STRINGENCY SILENT --REFERENCE_SEQUENCE /home/jrich/data/varseek_data/reference/ensembl_grch37_release93/Homo_sapiens.GRCh37.dna.primary_assembly.fa --ADD_PG_TAG_TO_READS true --PAIRED_RUN true --CLIP_ADAPTERS true --IS_BISULFITE_SEQUENCE false --ALIGNED_READS_ONLY false --MAX_INSERTIONS_OR_DELETIONS 1 --ATTRIBUTES_TO_REVERSE OQ --ATTRIBUTES_TO_REVERSE U2 --ATTRIBUTES_TO_REVERSE_COMPLEMENT E2 --ATTRIBUTES_TO_REVERSE_COMPLEMENT SQ --READ1_TRIM 0 --READ2_TRIM 0 --ALIGNER_PROPER_PAIR_FLAGS false --PRIMARY_ALIGNMENT_STRATEGY BestMapq --CLIP_OVERLAPPING_READS true --HARD_CLIP_OVERLAPPING_READS false --ADD_MATE_CIGAR true --UNMAP_CONTAMINANT_READS false --MIN_UNCLIPPED_BASES 32 --MATCHING_DICTIONARY_TAGS M5 --MATCHING_DICTIONARY_TAGS LN --UNMAPPED_READ_STRATEGY DO_NOT_CHANGE --VERBOSITY INFO --QUIET false --COMPRESSION_LEVEL 5 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false\n",
      "[Wed Nov 20 16:13:43 PST 2024] Executing as jrich@dator on Linux 3.10.0-1127.13.1.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 17.0.12+7; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:3.2.0-4-ge0475183b-SNAPSHOT\n",
      "INFO\t2024-11-20 16:13:43\tSamAlignmentMerger\tProcessing SAM file(s): [/home/jrich/data/varseek_data/gatk_nov17/alignment/sample_Aligned.sortedByCoord.out.bam]\n",
      "WARNING\t2024-11-20 16:13:43\tSamAlignmentMerger\tException merging bam alignment - attempting to sort aligned reads and try again: Underlying iterator is not queryname sorted: vcrs_4003213_141fW 150b aligned to 1:11317055-11317204. > vcrs_4003213_140fM 150b aligned to 1:11317056-11317205.\n",
      "INFO\t2024-11-20 16:13:44\tSamAlignmentMerger\tFinished reading 57409 total records from alignment SAM/BAM.\n",
      "INFO\t2024-11-20 16:13:52\tAbstractAlignmentMerger\tWrote 57409 alignment records and 0 unmapped reads.\n",
      "[Wed Nov 20 16:13:52 PST 2024] picard.sam.MergeBamAlignment done. Elapsed time: 0.15 minutes.\n",
      "Runtime.totalMemory()=2181038080\n"
     ]
    }
   ],
   "source": [
    "!$java -jar $picard_jar MergeBamAlignment \\\n",
    "    --ALIGNED_BAM $aligned_and_unmapped_bam \\\n",
    "    --UNMAPPED_BAM $unmapped_bam \\\n",
    "    --OUTPUT $merged_bam \\\n",
    "    --REFERENCE_SEQUENCE $reference_genome_fasta \\\n",
    "    --SORT_ORDER coordinate \\\n",
    "    --INCLUDE_SECONDARY_ALIGNMENTS false \\\n",
    "    --VALIDATION_STRINGENCY SILENT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. MarkDuplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:13:53.536 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/jrich/opt/picard/build/libs/picard.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "[Wed Nov 20 16:13:53 PST 2024] MarkDuplicates --INPUT /home/jrich/data/varseek_data/gatk_nov17/alignment/merged.bam --OUTPUT /home/jrich/data/varseek_data/gatk_nov17/alignment/marked_duplicates.bam --METRICS_FILE /home/jrich/data/varseek_data/gatk_nov17/alignment/marked_dup_metrics.txt --VALIDATION_STRINGENCY SILENT --CREATE_INDEX true --MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP 50000 --MAX_FILE_HANDLES_FOR_READ_ENDS_MAP 8000 --SORTING_COLLECTION_SIZE_RATIO 0.25 --TAG_DUPLICATE_SET_MEMBERS false --REMOVE_SEQUENCING_DUPLICATES false --TAGGING_POLICY DontTag --CLEAR_DT true --DUPLEX_UMI false --FLOW_MODE false --FLOW_DUP_STRATEGY FLOW_QUALITY_SUM_STRATEGY --USE_END_IN_UNPAIRED_READS false --USE_UNPAIRED_CLIPPED_END false --UNPAIRED_END_UNCERTAINTY 0 --UNPAIRED_START_UNCERTAINTY 0 --FLOW_SKIP_FIRST_N_FLOWS 0 --FLOW_Q_IS_KNOWN_END false --FLOW_EFFECTIVE_QUALITY_THRESHOLD 15 --ADD_PG_TAG_TO_READS true --REMOVE_DUPLICATES false --ASSUME_SORTED false --DUPLICATE_SCORING_STRATEGY SUM_OF_BASE_QUALITIES --PROGRAM_RECORD_ID MarkDuplicates --PROGRAM_GROUP_NAME MarkDuplicates --READ_NAME_REGEX <optimized capture of last three ':' separated fields as numeric values> --OPTICAL_DUPLICATE_PIXEL_DISTANCE 100 --MAX_OPTICAL_DUPLICATE_SET_SIZE 300000 --VERBOSITY INFO --QUIET false --COMPRESSION_LEVEL 5 --MAX_RECORDS_IN_RAM 500000 --CREATE_MD5_FILE false --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false\n",
      "[Wed Nov 20 16:13:53 PST 2024] Executing as jrich@dator on Linux 3.10.0-1127.13.1.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 17.0.12+7; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:3.2.0-4-ge0475183b-SNAPSHOT\n",
      "INFO\t2024-11-20 16:13:53\tMarkDuplicates\tStart of doWork freeMemory: 377797184; totalMemory: 402653184; maxMemory: 32178700288\n",
      "INFO\t2024-11-20 16:13:53\tMarkDuplicates\tReading input file and constructing read end information.\n",
      "INFO\t2024-11-20 16:13:53\tMarkDuplicates\tWill retain up to 116589493 data points before spilling to disk.\n",
      "WARNING\t2024-11-20 16:13:54\tAbstractOpticalDuplicateFinderCommandLineProgram\tA field field parsed out of a read name was expected to contain an integer and did not. Read name: vcrs_4003213_141fM. Cause: String 'vcrs_4003213_141fM' did not start with a parsable number.\n",
      "INFO\t2024-11-20 16:13:54\tMarkDuplicates\tRead 56616 records. 0 pairs never matched.\n",
      "INFO\t2024-11-20 16:13:54\tMarkDuplicates\tAfter buildSortedReadEndLists freeMemory: 937852776; totalMemory: 1912602624; maxMemory: 32178700288\n",
      "INFO\t2024-11-20 16:13:54\tMarkDuplicates\tWill retain up to 1005584384 duplicate indices before spilling to disk.\n",
      "INFO\t2024-11-20 16:13:56\tMarkDuplicates\tTraversing read pair information and detecting duplicates.\n",
      "INFO\t2024-11-20 16:13:56\tMarkDuplicates\tTraversing fragment information and detecting duplicates.\n",
      "INFO\t2024-11-20 16:13:56\tMarkDuplicates\tSorting list of duplicate records.\n",
      "INFO\t2024-11-20 16:13:56\tMarkDuplicates\tAfter generateDuplicateIndexes freeMemory: 7264612256; totalMemory: 15351152640; maxMemory: 32178700288\n",
      "INFO\t2024-11-20 16:13:56\tMarkDuplicates\tMarking 14535 records as duplicates.\n",
      "INFO\t2024-11-20 16:13:56\tMarkDuplicates\tFound 0 optical duplicate clusters.\n",
      "INFO\t2024-11-20 16:13:56\tMarkDuplicates\tReads are assumed to be ordered by: coordinate\n",
      "INFO\t2024-11-20 16:13:57\tMarkDuplicates\tWriting complete. Closing input iterator.\n",
      "INFO\t2024-11-20 16:13:57\tMarkDuplicates\tDuplicate Index cleanup.\n",
      "INFO\t2024-11-20 16:13:57\tMarkDuplicates\tGetting Memory Stats.\n",
      "INFO\t2024-11-20 16:13:57\tMarkDuplicates\tBefore output close freeMemory: 810335624; totalMemory: 838860800; maxMemory: 32178700288\n",
      "INFO\t2024-11-20 16:13:57\tMarkDuplicates\tClosed outputs. Getting more Memory Stats.\n",
      "INFO\t2024-11-20 16:13:57\tMarkDuplicates\tAfter output close freeMemory: 307062144; totalMemory: 335544320; maxMemory: 32178700288\n",
      "[Wed Nov 20 16:13:57 PST 2024] picard.sam.markduplicates.MarkDuplicates done. Elapsed time: 0.06 minutes.\n",
      "Runtime.totalMemory()=335544320\n"
     ]
    }
   ],
   "source": [
    "!$java -jar $picard_jar MarkDuplicates \\\n",
    "      --INPUT $merged_bam \\\n",
    "      --OUTPUT $marked_duplicates_bam \\\n",
    "      --METRICS_FILE $marked_dup_metrics_txt \\\n",
    "      --CREATE_INDEX true \\\n",
    "      --VALIDATION_STRINGENCY SILENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. SplitNCigarReads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GATK jar /home/jrich/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar\n",
      "Running:\n",
      "    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/jrich/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar SplitNCigarReads -R /home/jrich/data/varseek_data/reference/ensembl_grch37_release93/Homo_sapiens.GRCh37.dna.primary_assembly.fa -I /home/jrich/data/varseek_data/gatk_nov17/alignment/marked_duplicates.bam -O /home/jrich/data/varseek_data/gatk_nov17/alignment/split_n_cigar_reads.bam\n",
      "16:14:12.055 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/jrich/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "16:14:12.299 INFO  SplitNCigarReads - ------------------------------------------------------------\n",
      "16:14:12.304 INFO  SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.6.0.0\n",
      "16:14:12.304 INFO  SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/\n",
      "16:14:12.305 INFO  SplitNCigarReads - Executing as jrich@dator on Linux v3.10.0-1127.13.1.el7.x86_64 amd64\n",
      "16:14:12.305 INFO  SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v17.0.12+7\n",
      "16:14:12.306 INFO  SplitNCigarReads - Start Date/Time: November 20, 2024 at 4:14:11 PM PST\n",
      "16:14:12.306 INFO  SplitNCigarReads - ------------------------------------------------------------\n",
      "16:14:12.306 INFO  SplitNCigarReads - ------------------------------------------------------------\n",
      "16:14:12.307 INFO  SplitNCigarReads - HTSJDK Version: 4.1.1\n",
      "16:14:12.307 INFO  SplitNCigarReads - Picard Version: 3.2.0\n",
      "16:14:12.307 INFO  SplitNCigarReads - Built for Spark Version: 3.5.0\n",
      "16:14:12.307 INFO  SplitNCigarReads - HTSJDK Defaults.COMPRESSION_LEVEL : 2\n",
      "16:14:12.308 INFO  SplitNCigarReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false\n",
      "16:14:12.308 INFO  SplitNCigarReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true\n",
      "16:14:12.308 INFO  SplitNCigarReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false\n",
      "16:14:12.308 INFO  SplitNCigarReads - Deflater: IntelDeflater\n",
      "16:14:12.308 INFO  SplitNCigarReads - Inflater: IntelInflater\n",
      "16:14:12.309 INFO  SplitNCigarReads - GCS max retries/reopens: 20\n",
      "16:14:12.309 INFO  SplitNCigarReads - Requester pays: disabled\n",
      "16:14:12.309 INFO  SplitNCigarReads - Initializing engine\n",
      "16:14:12.477 INFO  SplitNCigarReads - Done initializing engine\n",
      "16:14:12.523 INFO  ProgressMeter - Starting traversal\n",
      "16:14:12.524 INFO  ProgressMeter -        Current Locus  Elapsed Minutes       Reads Processed     Reads/Minute\n",
      "16:14:13.681 INFO  SplitNCigarReads - 0 read(s) filtered by: AllowAllReadsReadFilter \n",
      "\n",
      "16:14:13.686 INFO  OverhangFixingManager - Overhang Fixing Manager saved 0 reads in the first pass\n",
      "16:14:13.690 INFO  SplitNCigarReads - Starting traversal pass 2\n",
      "16:14:14.556 INFO  SplitNCigarReads - 0 read(s) filtered by: AllowAllReadsReadFilter \n",
      "\n",
      "16:14:14.558 INFO  ProgressMeter -          X:154448493              0.0                113232        3341820.0\n",
      "16:14:14.558 INFO  ProgressMeter - Traversal complete. Processed 113232 total reads in 0.0 minutes.\n",
      "16:14:15.009 INFO  SplitNCigarReads - Shutting down engine\n",
      "[November 20, 2024 at 4:14:15 PM PST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 0.05 minutes.\n",
      "Runtime.totalMemory()=1224736768\n"
     ]
    }
   ],
   "source": [
    "_ = pysam.faidx(reference_genome_fasta)\n",
    "\n",
    "!$gatk SplitNCigarReads \\\n",
    "    -R $reference_genome_fasta \\\n",
    "    -I $marked_duplicates_bam \\\n",
    "    -O $split_n_cigar_reads_bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of DataFrame with tuples:\n",
      "Index      128\n",
      "col      56000\n",
      "dtype: int64\n",
      "\n",
      "Memory usage of DataFrame with NumPy arrays:\n",
      "Index       128\n",
      "col      144000\n",
      "dtype: int64\n",
      "\n",
      "Memory usage of DataFrame with sets:\n",
      "Index       128\n",
      "col      208000\n",
      "dtype: int64\n",
      "\n",
      "Memory usage of DataFrame with lists:\n",
      "Index      128\n",
      "col      72000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# DataFrame with 1000 tuples of length 1\n",
    "df_tuples = pd.DataFrame({\"col\": [(i,i+1,i+2) for i in range(1000)]})\n",
    "\n",
    "# DataFrame with 1000 NumPy arrays of length 1\n",
    "df_arrays = pd.DataFrame({\"col\": [np.array([i,i+1,i+2]) for i in range(1000)]})\n",
    "\n",
    "df_sets = pd.DataFrame({\"col\": [set([i,i+1,i+2]) for i in range(1000)]})\n",
    "\n",
    "df_lists = pd.DataFrame({\"col\": [[i,i+1,i+2] for i in range(1000)]})\n",
    "\n",
    "print(\"Memory usage of DataFrame with tuples:\")\n",
    "print(df_tuples.memory_usage(deep=True))\n",
    "\n",
    "print(\"\\nMemory usage of DataFrame with NumPy arrays:\")\n",
    "print(df_arrays.memory_usage(deep=True))\n",
    "\n",
    "print(\"\\nMemory usage of DataFrame with sets:\")\n",
    "print(df_sets.memory_usage(deep=True))\n",
    "\n",
    "print(\"\\nMemory usage of DataFrame with lists:\")\n",
    "print(df_lists.memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bus_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbus_df\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bus_df' is not defined"
     ]
    }
   ],
   "source": [
    "bus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. BaseRecalibrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  -I in old version, -F in new version\n",
    "# if not os.path.exists(f\"{ensembl_germline_vcf_filtered}.idx\"):\n",
    "#     !$gatk IndexFeatureFile -I $ensembl_germline_vcf_filtered  # TODO: ~81092000/324780532 processed before error - current error being a duplicate G in REF/ALT column\n",
    "\n",
    "if not os.path.exists(f\"{genomes1000_vcf}.idx\"):\n",
    "    !$gatk IndexFeatureFile -I $genomes1000_vcf\n",
    "\n",
    "# TODO: uncomment once I install bgzip\n",
    "# if not os.path.exists(f\"{panel_of_normals_vcf_filtered}.idx\"):\n",
    "#     !gatk IndexFeatureFile -I $panel_of_normals_vcf_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GATK jar /home/jrich/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar\n",
      "Running:\n",
      "    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/jrich/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar BaseRecalibrator -I /home/jrich/data/varseek_data/gatk_nov16/alignment/split_n_cigar_reads.bam -R /home/jrich/data/varseek_data/reference/ensembl_grch37_release93/Homo_sapiens.GRCh37.dna.primary_assembly.fa --use-original-qualities --known-sites /home/jrich/data/varseek_data/reference/ensembl_grch37_release93/1000GENOMES-phase_3.vcf -O /home/jrich/data/varseek_data/gatk_nov16/alignment/recal_data.table\n",
      "Error: LinkageError occurred while loading main class org.broadinstitute.hellbender.Main\n",
      "\tjava.lang.UnsupportedClassVersionError: org/broadinstitute/hellbender/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 55.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: when I fix !$gatk IndexFeatureFile -I $ensembl_germline_vcf_filtered, add back --known-sites $ensembl_germline_vcf_filtered\n",
    "!$gatk BaseRecalibrator \\\n",
    "    -I $split_n_cigar_reads_bam \\\n",
    "    -R $reference_genome_fasta \\\n",
    "    --use-original-qualities \\\n",
    "    --known-sites $genomes1000_vcf \\\n",
    "    -O $recal_data_table\n",
    "\n",
    "# -known-sites ${dbSNP_vcf} \\\n",
    "# -known-sites ${sep=\" --known-sites \" known_indels_sites_VCFs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Apply Recalibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GATK jar /home/jrich/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar\n",
      "Running:\n",
      "    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/jrich/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar ApplyBQSR --add-output-sam-program-record -R /home/jrich/data/varseek_data/reference/ensembl_grch37_release93/Homo_sapiens.GRCh37.dna.primary_assembly.fa -I /home/jrich/data/varseek_data/gatk_nov16/alignment/split_n_cigar_reads.bam --use-original-qualities --bqsr-recal-file /home/jrich/data/varseek_data/gatk_nov16/alignment/recal_data.table -O /home/jrich/data/varseek_data/gatk_nov16/alignment/recalibrated.bam\n",
      "Error: LinkageError occurred while loading main class org.broadinstitute.hellbender.Main\n",
      "\tjava.lang.UnsupportedClassVersionError: org/broadinstitute/hellbender/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 55.0\n"
     ]
    }
   ],
   "source": [
    "!$gatk ApplyBQSR \\\n",
    "    --add-output-sam-program-record \\\n",
    "    -R $reference_genome_fasta \\\n",
    "    -I $split_n_cigar_reads_bam \\\n",
    "    --use-original-qualities \\\n",
    "    --bqsr-recal-file $recal_data_table \\\n",
    "    -O $recalibrated_bam\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. AnalyzeCovariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GATK jar /home/jrich/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar\n",
      "Running:\n",
      "    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/jrich/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar AnalyzeCovariates -bqsr /home/jrich/data/varseek_data/gatk_nov16/alignment/recal_data.table -plots /home/jrich/data/varseek_data/gatk_nov16/alignment/AnalyzeCovariates.pdf\n",
      "Error: LinkageError occurred while loading main class org.broadinstitute.hellbender.Main\n",
      "\tjava.lang.UnsupportedClassVersionError: org/broadinstitute/hellbender/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 55.0\n"
     ]
    }
   ],
   "source": [
    "!$gatk AnalyzeCovariates \\\n",
    "    -bqsr $recal_data_table \\\n",
    "    -plots $covariates_plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8a. HaplotypeCaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GATK jar /home/jrich/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar\n",
      "Running:\n",
      "    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /home/jrich/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar HaplotypeCaller -R /home/jrich/data/varseek_data/reference/ensembl_grch37_release93/Homo_sapiens.GRCh37.dna.primary_assembly.fa -I /home/jrich/data/varseek_data/gatk_nov16/alignment/recalibrated.bam -O /home/jrich/data/varseek_data/gatk_nov16/vcfs/haplotypecaller/haplotypecaller_output_unfiltered.g.vcf.gz --standard-min-confidence-threshold-for-calling 20 -dont-use-soft-clipped-bases\n",
      "Error: LinkageError occurred while loading main class org.broadinstitute.hellbender.Main\n",
      "\tjava.lang.UnsupportedClassVersionError: org/broadinstitute/hellbender/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 55.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!$gatk --java-options \"-Xmx4g\" HaplotypeCaller  \\\n",
    "    -R $reference_genome_fasta \\\n",
    "    -I $recalibrated_bam \\\n",
    "    -O $haplotypecaller_unfiltered_vcf \\\n",
    "    --standard-min-confidence-threshold-for-calling 20 \\\n",
    "    -dont-use-soft-clipped-bases \\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.5a. MergeVcfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_vcf = haplotypecaller_merged.vcf\n",
    "# \n",
    "# !$java -jar $picard_jar MergeVcfs \\\n",
    "#     --INPUT $vcf1 \\\n",
    "#     --INPUT $vcf2 \\\n",
    "#     --OUTPUT $merged_vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9a. VariantFiltration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GATK jar /home/jrich/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar\n",
      "Running:\n",
      "    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/jrich/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar VariantFiltration -R /home/jrich/data/varseek_data/reference/ensembl_grch37_release93/Homo_sapiens.GRCh37.dna.primary_assembly.fa -V /home/jrich/data/varseek_data/gatk_nov16/vcfs/haplotypecaller/haplotypecaller_output_unfiltered.g.vcf.gz -O /home/jrich/data/varseek_data/gatk_nov16/vcfs/haplotypecaller/haplotypecaller_output_filtered.vcf.gz --window 35 --cluster 3 --filter-name FS --filter FS > 30.0 --filter-name QD --filter QD < 2.0\n",
      "Error: LinkageError occurred while loading main class org.broadinstitute.hellbender.Main\n",
      "\tjava.lang.UnsupportedClassVersionError: org/broadinstitute/hellbender/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 55.0\n"
     ]
    }
   ],
   "source": [
    "# cosmic_vcf = \"\"\n",
    "\n",
    "!$gatk VariantFiltration \\\n",
    "    -R $reference_genome_fasta \\\n",
    "    -V $haplotypecaller_unfiltered_vcf \\\n",
    "    -O $haplotypecaller_filtered_vcf \\\n",
    "    --window 35 \\\n",
    "    --cluster 3 \\\n",
    "    --filter-name \"FS\" \\\n",
    "    --filter \"FS > 30.0\" \\\n",
    "    --filter-name \"QD\" \\\n",
    "    --filter \"QD < 2.0\"\n",
    "\n",
    "    # --mask $cosmic_vcf \\\n",
    "    # --mask-name \"COSMIC\" \\\n",
    "    # --filter-not-in-mask \\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10a. Do the filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GATK jar /home/jrich/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar\n",
      "Running:\n",
      "    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/jrich/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar SelectVariants -V /home/jrich/data/varseek_data/gatk_nov16/vcfs/haplotypecaller/haplotypecaller_output_filtered.vcf.gz --exclude-filtered true -O /home/jrich/data/varseek_data/gatk_nov16/vcfs/haplotypecaller/haplotypecaller_output_filtered_applied.vcf.gz\n",
      "Error: LinkageError occurred while loading main class org.broadinstitute.hellbender.Main\n",
      "\tjava.lang.UnsupportedClassVersionError: org/broadinstitute/hellbender/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 55.0\n"
     ]
    }
   ],
   "source": [
    "!$gatk SelectVariants \\\n",
    "     -V $haplotypecaller_filtered_vcf \\\n",
    "     --exclude-filtered true \\\n",
    "     -O $haplotypecaller_filtered_applied_vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8b. Mutect2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GATK jar /home/jrich/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar\n",
      "Running:\n",
      "    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/jrich/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar Mutect2 -R /home/jrich/data/varseek_data/reference/ensembl_grch37_release93/Homo_sapiens.GRCh37.dna.primary_assembly.fa -I /home/jrich/data/varseek_data/gatk_nov16/alignment/recalibrated.bam -O /home/jrich/data/varseek_data/gatk_nov16/vcfs/mutect2/mutect2_output_unfiltered.g.vcf.gz --min-base-quality-score 20\n",
      "Error: LinkageError occurred while loading main class org.broadinstitute.hellbender.Main\n",
      "\tjava.lang.UnsupportedClassVersionError: org/broadinstitute/hellbender/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 55.0\n"
     ]
    }
   ],
   "source": [
    "# consider adding --disable-read-filter\n",
    "# TODO: add --panel-of-normals $panel_of_normals_vcf_filtered once I install bgzip\n",
    "!$gatk Mutect2 \\\n",
    "    -R $reference_genome_fasta \\\n",
    "    -I $recalibrated_bam \\\n",
    "    -O $mutect2_unfiltered_vcf \\\n",
    "    --min-base-quality-score 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9b. FilterMutectCalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GATK jar /home/jrich/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar\n",
      "Running:\n",
      "    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/jrich/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar FilterMutectCalls -R /home/jrich/data/varseek_data/reference/ensembl_grch37_release93/Homo_sapiens.GRCh37.dna.primary_assembly.fa -V /home/jrich/data/varseek_data/gatk_nov16/vcfs/mutect2/mutect2_output_unfiltered.g.vcf.gz -O /home/jrich/data/varseek_data/gatk_nov16/vcfs/mutect2/mutect2_output_filtered.vcf.gz\n",
      "Error: LinkageError occurred while loading main class org.broadinstitute.hellbender.Main\n",
      "\tjava.lang.UnsupportedClassVersionError: org/broadinstitute/hellbender/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 55.0\n"
     ]
    }
   ],
   "source": [
    "# # if multiple stats files:\n",
    "# !$gatk MergeMutectStats -stats unfiltered1.vcf.stats -stats unfiltered2.vcf.stats -O unfiltered.vcf.stats\n",
    "\n",
    "# stats_file = f\"{mutect2_unfiltered_vcf}.stats\"\n",
    "# --stats $stats_file\n",
    "!$gatk FilterMutectCalls \\\n",
    "    -R $reference_genome_fasta \\\n",
    "    -V $mutect2_unfiltered_vcf \\\n",
    "    -O $mutect2_filtered_vcf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_sample_vcf(output_path):\n",
    "#     with open(output_path, \"w\") as vcf_file:\n",
    "#         # Write VCF headers\n",
    "#         vcf_file.write(\"##fileformat=VCFv4.2\\n\")\n",
    "#         vcf_file.write(\"##source=SampleVCFGenerator\\n\")\n",
    "#         vcf_file.write(\"##reference=GRCh37\\n\")\n",
    "#         vcf_file.write(\"##INFO=<ID=DP,Number=1,Type=Integer,Description=\\\"Total Depth\\\">\\n\")\n",
    "#         vcf_file.write(\"##INFO=<ID=AF,Number=A,Type=Float,Description=\\\"Allele Frequency\\\">\\n\")\n",
    "#         vcf_file.write(\"##FILTER=<ID=PASS,Description=\\\"All filters passed\\\">\\n\")\n",
    "#         vcf_file.write(\"##FORMAT=<ID=GT,Number=1,Type=String,Description=\\\"Genotype\\\">\\n\")\n",
    "#         vcf_file.write(\"#CHROM\\tPOS\\tID\\tREF\\tALT\\tQUAL\\tFILTER\\tINFO\\tFORMAT\\tsample1\\n\")\n",
    "        \n",
    "#         # Write sample variant entries\n",
    "#         variants = [\n",
    "#             (\"1\", 123456, \".\", \"G\", \"A\", 50, \"PASS\", \"DP=100;AF=0.5\", \"GT\", \"0/1\"),\n",
    "#             (\"1\", 234567, \".\", \"C\", \"T\", 60, \"PASS\", \"DP=200;AF=0.3\", \"GT\", \"1/1\"),\n",
    "#             (\"2\", 345678, \".\", \"T\", \"G\", 70, \"PASS\", \"DP=150;AF=0.1\", \"GT\", \"0/1\"),\n",
    "#             (\"X\", 456789, \".\", \"A\", \"C\", 80, \"PASS\", \"DP=120;AF=0.05\", \"GT\", \"0/0\")\n",
    "#         ]\n",
    "        \n",
    "#         for chrom, pos, var_id, ref, alt, qual, fltr, info, fmt, sample in variants:\n",
    "#             vcf_file.write(f\"{chrom}\\t{pos}\\t{var_id}\\t{ref}\\t{alt}\\t{qual}\\t{fltr}\\t{info}\\t{fmt}\\t{sample}\\n\")\n",
    "\n",
    "# # Usage\n",
    "# create_sample_vcf(\"/home/jrich/data/varseek_data/gatk/sample.vcf\")\n",
    "\n",
    "# df_sample = vcf_to_dataframe(\"/home/jrich/data/varseek_data/gatk/sample.vcf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JUMP TO HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E::hts_open_format] Failed to open file \"/home/jrich/data/varseek_data/gatk_nov16/vcfs/haplotypecaller/haplotypecaller_output_filtered_applied.vcf.gz\" : No such file or directory\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] could not open variant file `b'/home/jrich/data/varseek_data/gatk_nov16/vcfs/haplotypecaller/haplotypecaller_output_filtered_applied.vcf.gz'`: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvarseek\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vcf_to_dataframe\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Convert VCF to DataFrame\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df_hap \u001b[38;5;241m=\u001b[39m \u001b[43mvcf_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhaplotypecaller_filtered_applied_vcf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_columns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m df_mut \u001b[38;5;241m=\u001b[39m vcf_to_dataframe(mutect2_filtered_vcf, additional_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/varseek/varseek/utils/seq_utils.py:6063\u001b[0m, in \u001b[0;36mvcf_to_dataframe\u001b[0;34m(vcf_file, additional_columns)\u001b[0m\n\u001b[1;32m   6061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvcf_to_dataframe\u001b[39m(vcf_file, additional_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   6062\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a VCF file to a Pandas DataFrame.\"\"\"\u001b[39;00m    \n\u001b[0;32m-> 6063\u001b[0m     vcf \u001b[38;5;241m=\u001b[39m \u001b[43mpysam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariantFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvcf_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6065\u001b[0m     \u001b[38;5;66;03m# List to store VCF rows\u001b[39;00m\n\u001b[1;32m   6066\u001b[0m     vcf_data \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/varseek2/lib/python3.10/site-packages/pysam/libcbcf.pyx:4117\u001b[0m, in \u001b[0;36mpysam.libcbcf.VariantFile.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/varseek2/lib/python3.10/site-packages/pysam/libcbcf.pyx:4342\u001b[0m, in \u001b[0;36mpysam.libcbcf.VariantFile.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] could not open variant file `b'/home/jrich/data/varseek_data/gatk_nov16/vcfs/haplotypecaller/haplotypecaller_output_filtered_applied.vcf.gz'`: No such file or directory"
     ]
    }
   ],
   "source": [
    "from varseek.utils import vcf_to_dataframe\n",
    "\n",
    "# Convert VCF to DataFrame\n",
    "df_hap = vcf_to_dataframe(haplotypecaller_filtered_applied_vcf, additional_columns = False)\n",
    "df_mut = vcf_to_dataframe(mutect2_filtered_vcf, additional_columns = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmic_df = add_vcf_info_to_cosmic_tsv(cosmic_tsv=cosmic_tsv, reference_genome_fasta=reference_genome_fasta, cosmic_df_out = None, cosmic_cdna_info_csv = cosmic_cdna_info_csv, mutation_source = \"cdna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_188830/1989552023.py:2: DtypeWarning: Columns (21,36,38,49,50,54,56) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  unique_mcrs_df = pd.read_csv(unique_mcrs_df_path)\n"
     ]
    }
   ],
   "source": [
    "# load in unique_mcrs_df\n",
    "unique_mcrs_df = pd.read_csv(unique_mcrs_df_path)\n",
    "unique_mcrs_df.rename(columns={'TP': 'TP_vk', 'FP': 'FP_vk', 'TN': 'TN_vk', 'FN': 'FN_vk'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = cosmic_df[(cosmic_df['POS'].astype(int) >= 3699230) & (cosmic_df['POS'].astype(int) <= 3699239)]\n",
    "# filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmic_tp = unique_mcrs_df.loc[unique_mcrs_df['TP_vk']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'CHROM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_188830/2360849419.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#  take the intersection of COSMIC and STAR dfs based on CHROM, POS, REF, ALT - but keep the ID from the COSMIC vcf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m mut_cosmic_merged_df = pd.merge(df_mut, cosmic_df, \n\u001b[0m\u001b[1;32m      3\u001b[0m                      \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CHROM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'POS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'REF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ALT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                      \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      suffixes=('_df1', '_df2'))\n",
      "\u001b[0;32m~/miniconda3/envs/varseek2/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mindicator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m ) -> DataFrame:\n\u001b[0;32m--> 110\u001b[0;31m     op = _MergeOperation(\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/varseek2/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    699\u001b[0m         (\n\u001b[1;32m    700\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;31m# to avoid incompatible dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/varseek2/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m                         \u001b[0mleft_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1180\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                         \u001b[0;31m# work-around for merge_asof(left_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/varseek2/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m             )\n\u001b[1;32m   1849\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'CHROM'"
     ]
    }
   ],
   "source": [
    "#  take the intersection of COSMIC and STAR dfs based on CHROM, POS, REF, ALT - but keep the ID from the COSMIC vcf\n",
    "mut_cosmic_merged_df = pd.merge(df_mut, cosmic_df, \n",
    "                     on=['CHROM', 'POS', 'REF', 'ALT'], \n",
    "                     how='inner',\n",
    "                     suffixes=('_df1', '_df2'))\n",
    "\n",
    "mut_cosmic_merged_df = mut_cosmic_merged_df.drop(columns=['ID_df1']).rename(columns={'ID_df2': 'ID'})\n",
    "\n",
    "id_set_mut = set(mut_cosmic_merged_df['ID'])\n",
    "\n",
    "\n",
    "\n",
    "hap_cosmic_merged_df = pd.merge(df_hap, cosmic_df, \n",
    "                     on=['CHROM', 'POS', 'REF', 'ALT'], \n",
    "                     how='inner',\n",
    "                     suffixes=('_df1', '_df2'))\n",
    "\n",
    "hap_cosmic_merged_df = hap_cosmic_merged_df.drop(columns=['ID_df1']).rename(columns={'ID_df2': 'ID'})\n",
    "\n",
    "id_set_hap = set(hap_cosmic_merged_df['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_mutations_mutect = len(df_mut)\n",
    "number_of_cosmic_mutations_mutect = len(mut_cosmic_merged_df)\n",
    "number_of_mutations_haplotypecaller = len(df_hap)\n",
    "number_of_cosmic_mutations_haplotypecaller = len(hap_cosmic_merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_mcrs_df['mutation_detected_gatk_mutect2'] = unique_mcrs_df['reference_header'].isin(id_set_mut)  #!!! ensure 'reference_header' is correct here  # keep in mind that my IDs are the mutation headers (ENST...), NOT mcrs headers or mcrs ids\n",
    "\n",
    "unique_mcrs_df['TP'] = (unique_mcrs_df['included_in_synthetic_reads_mutant'] & unique_mcrs_df['mutation_detected_gatk_mutect2'])\n",
    "unique_mcrs_df['FP'] = (~unique_mcrs_df['included_in_synthetic_reads_mutant'] & unique_mcrs_df['mutation_detected_gatk_mutect2'])\n",
    "unique_mcrs_df['FN'] = (unique_mcrs_df['included_in_synthetic_reads_mutant'] & ~unique_mcrs_df['mutation_detected_gatk_mutect2'])\n",
    "unique_mcrs_df['TN'] = (~unique_mcrs_df['included_in_synthetic_reads_mutant'] & ~unique_mcrs_df['mutation_detected_gatk_mutect2'])\n",
    "\n",
    "mutect_stat_path = f\"{gatk_parent}/reference_metrics_mutect2.txt\"\n",
    "metric_dictionary_reference = calculate_metrics(unique_mcrs_df, header_name = \"mcrs_header\", check_assertions = False, out = mutect_stat_path)\n",
    "draw_confusion_matrix(metric_dictionary_reference)\n",
    "\n",
    "true_set = set(unique_mcrs_df.loc[unique_mcrs_df['included_in_synthetic_reads_mutant'], 'mcrs_header'])\n",
    "positive_set = set(unique_mcrs_df.loc[unique_mcrs_df['mutation_detected_gatk_mutect2'], 'mcrs_header'])\n",
    "create_venn_diagram(true_set, positive_set, TN = metric_dictionary_reference['TN'], out_path = f\"{gatk_parent}/venn_diagram_reference_cosmic_only_mutect2.png\")\n",
    "\n",
    "\n",
    "noncosmic_mutation_id_set = {f'mutect_fp_{i}' for i in range(1, number_of_mutations_mutect - number_of_cosmic_mutations_mutect + 1)}\n",
    "positive_set_including_noncosmic_mutations = positive_set.union(noncosmic_mutation_id_set)\n",
    "\n",
    "FP_including_noncosmic = metric_dictionary_reference['FP'] + len(positive_set_including_noncosmic_mutations)\n",
    "accuracy, sensitivity, specificity = calculate_sensitivity_specificity(metric_dictionary_reference['TP'], metric_dictionary_reference['TP'], FP_including_noncosmic, metric_dictionary_reference['TP'])\n",
    "\n",
    "with open(mutect_stat_path, \"a\") as file:\n",
    "    file.write(f\"FP including non-cosmic: {FP_including_noncosmic}\\n\")\n",
    "    file.write(f\"accuracy including non-cosmic: {accuracy}\\n\")\n",
    "    file.write(f\"specificity including non-cosmic: {specificity}\\n\")\n",
    "\n",
    "create_venn_diagram(true_set, positive_set_including_noncosmic_mutations, TN = metric_dictionary_reference['TN'], out_path = f\"{gatk_parent}/venn_diagram_reference_including_noncosmics_mutect2.png\")\n",
    "\n",
    "create_stratified_metric_bar_plot(unique_mcrs_df, 'number_of_reads_mutant', 'accuracy', overall_metric = metric_dictionary_reference['accuracy'], log_x_axis = False, out_path = f\"{plot_output_folder}/accuracy_vs_number_of_reads_mutant.png\")\n",
    "#!!! create similar plots for y in {sensitivity, specificity}, and x in {number_of_reads_wt, tumor_purity} and determine cutoffs for which GATK is reliable\n",
    "\n",
    "unique_mcrs_df.rename(columns={'TP': 'TP_gatk_mutect2', 'FP': 'FP_gatk_mutect2', 'TN': 'TN_gatk_mutect2', 'FN': 'FN_gatk_mutect2'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_mcrs_df['mutation_detected_gatk_haplotypecaller'] = unique_mcrs_df['reference_header'].isin(id_set_hap)\n",
    "\n",
    "unique_mcrs_df['TP'] = (unique_mcrs_df['included_in_synthetic_reads_mutant'] & unique_mcrs_df['mutation_detected_gatk_haplotypecaller'])\n",
    "unique_mcrs_df['FP'] = (~unique_mcrs_df['included_in_synthetic_reads_mutant'] & unique_mcrs_df['mutation_detected_gatk_haplotypecaller'])\n",
    "unique_mcrs_df['FN'] = (unique_mcrs_df['included_in_synthetic_reads_mutant'] & ~unique_mcrs_df['mutation_detected_gatk_haplotypecaller'])\n",
    "unique_mcrs_df['TN'] = (~unique_mcrs_df['included_in_synthetic_reads_mutant'] & ~unique_mcrs_df['mutation_detected_gatk_haplotypecaller'])\n",
    "\n",
    "haplotypecaller_stat_path = f\"{gatk_parent}/reference_metrics_haplotypecaller.txt\"\n",
    "metric_dictionary_reference = calculate_metrics(unique_mcrs_df, header_name = \"mcrs_header\", check_assertions = False, out = haplotypecaller_stat_path)\n",
    "draw_confusion_matrix(metric_dictionary_reference)\n",
    "\n",
    "true_set = set(unique_mcrs_df.loc[unique_mcrs_df['included_in_synthetic_reads_mutant'], 'mcrs_header'])\n",
    "positive_set = set(unique_mcrs_df.loc[unique_mcrs_df['mutation_detected_gatk_haplotypecaller'], 'mcrs_header'])\n",
    "create_venn_diagram(true_set, positive_set, TN = metric_dictionary_reference['TN'], out_path = f\"{gatk_parent}/venn_diagram_reference_cosmic_only_haplotypecaller.png\")\n",
    "\n",
    "\n",
    "\n",
    "noncosmic_mutation_id_set = {f'haplotypecaller_fp_{i}' for i in range(1, number_of_mutations_haplotypecaller - number_of_cosmic_mutations_haplotypecaller + 1)}\n",
    "positive_set_including_noncosmic_mutations = positive_set.union(noncosmic_mutation_id_set)\n",
    "\n",
    "FP_including_noncosmic = metric_dictionary_reference['FP'] + len(positive_set_including_noncosmic_mutations)\n",
    "accuracy, sensitivity, specificity = calculate_sensitivity_specificity(metric_dictionary_reference['TP'], metric_dictionary_reference['TP'], FP_including_noncosmic, metric_dictionary_reference['TP'])\n",
    "\n",
    "with open(haplotypecaller_stat_path, \"a\") as file:\n",
    "    file.write(f\"FP including non-cosmic: {FP_including_noncosmic}\\n\")\n",
    "    file.write(f\"accuracy including non-cosmic: {accuracy}\\n\")\n",
    "    file.write(f\"specificity including non-cosmic: {specificity}\\n\")\n",
    "\n",
    "create_venn_diagram(true_set, positive_set_including_noncosmic_mutations, TN = metric_dictionary_reference['TN'], out_path = f\"{gatk_parent}/venn_diagram_reference_including_noncosmics_haplotypecaller.png\")\n",
    "\n",
    "create_stratified_metric_bar_plot(unique_mcrs_df, 'number_of_reads_mutant', 'accuracy', overall_metric = metric_dictionary_reference['accuracy'], log_x_axis = False, out_path = f\"{plot_output_folder}/accuracy_vs_number_of_reads_mutant.png\")\n",
    "#!!! create similar plots for y in {sensitivity, specificity}, and x in {number_of_reads_wt, tumor_purity} and determine cutoffs for which GATK is reliable\n",
    "\n",
    "unique_mcrs_df.rename(columns={'TP': 'TP_gatk_haplotypecaller', 'FP': 'FP_gatk_haplotypecaller', 'TN': 'TN_gatk_haplotypecaller', 'FN': 'FN_gatk_haplotypecaller'}, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "varseek2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
